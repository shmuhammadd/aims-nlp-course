#  **Introduction to NLP â€“ Course Overview**  

Welcome to the **Introduction to NLP** course! Over the next **3 weeks**, weâ€™ll take a deep dive into **Natural Language Processing (NLP)**â€”from fundamental text processing techniques to cutting-edge **deep learning models like transformers**.  

This course is designed for **beginners and intermediate learners** who want to build a solid understanding of NLP, gain **hands-on experience**, and work with **real-world applications**.  

By the end of this course, you will be able to:  
âœ… Understand the **NLP pipeline** and essential text processing techniques  
âœ… Implement **statistical and deep learning-based NLP models**  
âœ… Work with **word embeddings, transformers, and transfer learning**  
âœ… Use **PyTorch & Hugging Face** to build state-of-the-art NLP applications  
âœ… Learn about **ethical considerations in NLP** and the **future of AI models**  

---

# **Course Schedule**

## ðŸ”¹ Week 1: Foundations of NLP & Classical Approaches  

| **Day**  | **Slide Topics** | **Reading Materials** |
|----------|-----------------|-----------------------|
|      18/03/2025    | Course Introduction & NLP Pipeline | [Speech and Language Processing](https://github.com/rain1024/slp2-pdf/tree/master/chapter-wise-pdf) - Ch.1, <br> [Natural Language Processing: State of the Art, Current Trends and Challenges](https://link.springer.com/article/10.1007/s11042-022-13428-4)  |
|  18/03/2025         | Text Preprocessing & Tokenization | 1.  [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/2.pdf) - Ch.1  <br> 2. [NLTK Book](https://www.nltk.org/book/) - Ch.3, <br>  3. [spaCy Docs](https://spacy.io/usage) |
|  19/03/2025         | Statistical Language Models (n-grams) | [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/3.pdf) - Ch.3     |    
|  20/03/2025         | Text Classification | [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/4.pdf) - Ch.4     |    
|          | Word Embeddings (Word2Vec, fastText, GloVe) | |
|          | Introduction to Neural Networks | |
|          | RNNs, LSTMs, GRUs | |
|          | Sequence-to-Sequence Models & Decoding | |


##  PyTorch Tutorial

PyTorch is extensively used in research and production due to its ease of use and powerful capabilities. To get started, follow the tutorial available at the link below:

[**PyTorch Tutorial (Google Colab)**](https://colab.research.google.com/drive/1-iiPAYiAWfZc0Mxz1FLLL0_KmLdegFaQ#scrollTo=s2jvK3g_2zrK)


###  Assignment 1: PyTorch Essentials

After completing the tutorial, students are required to practice and apply their understanding by working on the following two exercises. These exercises will help reinforce the fundamental concepts of PyTorch
and its workflow in deep learning applications.

####  **Assignment 1: PyTorch Fundamentals**
- [**PyTorch Fundamentals Exercises**](https://github.com/shmuhammadd/aims-nlp-course/blob/main/Assignment_1/00_pytorch_fundamentals_exercises.ipynb)  
  *(Tensors, operations, GPU acceleration).*

####  **Assignment 2: PyTorch Workflow**
- [**PyTorch Workflow Exercises**](https://github.com/shmuhammadd/aims-nlp-course/blob/main/Assignment_1/01_pytorch_workflow_exercises.ipynb)  
  *(Model training, optimization, evaluation).*


---

## ðŸ”¹ Week 2: Transformers, Transfer Learning & Fine-Tuning  

| **Day**  | **Slide Topics** | **Reading Materials** |
|----------|-----------------|-----------------------|
|          | Introduction to Transformers | |
|          | Building Transformers from Scratch | |
|          | Pretrained Language Models (BERT, GPT, T5) | |
|          | Fine-Tuning NLP Models | |
|          | Prompt Engineering & In-Context Learning | |
|          | Reinforcement Learning for NLP (RLHF) | |
|          | Mid-Course Project (Fine-Tune a Model) | |

---

## ðŸ”¹ Week 3: Advanced Topics, Knowledge Graphs & Ethical NLP  

| **Day**  | **Slide Topics** | **Reading Materials** |
|----------|-----------------|-----------------------|
|          | Efficient Adaptation of Large Models | |
|          | Interpretability & Explainability in NLP | |
|          | Knowledge Graphs & Graph Neural Networks | |
|          | Retrieval-Augmented Generation (RAG) | |
|          | Future of NLP & Large-Scale Models | |
|          | Ethical Considerations in NLP | |
|          | Final Project & Course Wrap-Up | |



---

#  **Prerequisites**  
- Basic knowledge of **Python & Machine Learning**  
- Some familiarity with **linear algebra, probability, and statistics** (preferred but not required)  

---

# **Additional Books & Resources**  

## **Books**  
1. **Speech and Language Processing** â€“ Jurafsky & Martin ([Online Draft](https://web.stanford.edu/~jurafsky/slp3/))  
2. **Natural Language Processing with Python** â€“ Steven Bird, Ewan Klein, Edward Loper ([Free Online](https://www.nltk.org/book/))  
3. **Transformers for Natural Language Processing** â€“ Denis Rothman  
4. **Deep Learning for NLP** â€“ Palash Goyal, Sumit Pandey, Karan Jain  
5. **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** â€“ AurÃ©lien GÃ©ron  

