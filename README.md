# Natural Language Processing & Large Language Models  
---

## ðŸ“‘ Course Syllabus

| Lecture | Title | Topics / Activities | Materials |
|---------|-------|---------------------|-----------|
| **1** | Introduction to NLP and LLMs | What is NLP? Evolution: n-grams â†’ RNNs â†’ Transformers â†’ LLMs. Applications and motivations. Course structure & tools. | [Assignment](#) Â· [Code](#) |
| **2** | Text Processing & Linguistic Foundations | Tokenization, normalization, subword models (BPE, SentencePiece). Morphology, syntax, semantics. Classical reps: BoW, TF-IDF, n-grams. | [Assignment](#) Â· [Code](#) |
| **3** | Word Embeddings & Distributional Semantics | Word2Vec, GloVe, FastText. Distributional hypothesis. Intrinsic vs extrinsic evaluation. Hands-on: embedding visualization. | [Assignment](#) Â· [Code](#) |
| **4** | Neural Sequence Models | RNNs, LSTMs, GRUs. Encoderâ€“decoder architectures. Attention as solution to long dependencies. | [Assignment](#) Â· [Code](#) |
| **5** | The Transformer Model | Self-attention & multi-head attention. Positional encodings. Encoder/decoder designs. Hands-on: mini-transformer from scratch. | [Assignment](#) Â· [Code](#) |
| **6** | Training Large Language Models | Pretraining objectives (LM, MLM, denoising). Datasets & tokenization. Optimizers, scaling laws, hardware. | [Assignment](#) Â· [Code](#) |
| **7** | Fine-Tuning & Adaptation | Supervised fine-tuning. LoRA, QLoRA, adapters. Domain adaptation. Hands-on: fine-tune a small model. | [Assignment](#) Â· [Code](#) |
| **8** | Prompting & Instruction Tuning | Prompt engineering. Zero/few-shot, chain-of-thought prompting. Instruction-tuned models (T5, FLAN, Alpaca). | [Assignment](#) Â· [Code](#) |
| **9** | Alignment & RLHF | Human feedback in training loops. Reward modeling & PPO. Safety and alignment challenges. | [Assignment](#) Â· [Code](#) |
| **10** | Evaluation of LLMs | Intrinsic metrics (perplexity, BLEU, ROUGE). Benchmarks (GLUE, SuperGLUE, MMLU). Human eval. Detecting hallucinations/bias. | [Assignment](#) Â· [Code](#) |
| **11** | Model Efficiency & Compression | Quantization (8-bit, 4-bit), pruning, distillation. Flash attention. Deployment tradeoffs. Hands-on: quantize a model. | [Assignment](#) Â· [Code](#) |
| **12** | Retrieval-Augmented Generation (RAG) | Dense embeddings & vector databases. Hybrid retrieval. Applications: knowledge grounding, chatbots. Hands-on: toy RAG pipeline. | [Assignment](#) Â· [Code](#) |
| **13** | Agents, Tools & Multimodal LLMs | LLM agents (LangChain, HuggingFace). Tool use & APIs. Multimodal LLMs (text+image, text+speech). Hands-on: connect LLM with API. | [Assignment](#) Â· [Code](#) |
| **14** | Deployment, Production & Ethics | Serving models (FastAPI, HF Inference). Scaling & monitoring. Ethics: bias, privacy, misuse, prompt injection. | [Assignment](#) Â· [Code](#) |
| **15** | Future Directions & Student Presentations | Mixture of Experts, model merging, continual learning. Open-source vs proprietary race. Student project demos. Wrap-up. | [Assignment](#) Â· [Code](#) |

---


-  Working with Text Data
-  Ch 1: Understanding Large Language Models	No code	-
-  Ch 2: Working with Text Data	- ch02.ipynb
-  Ch 3: Coding Attention Mechanisms	- ch03.ipynb
-  Ch 4: Implementing a GPT Model from Scratch	- ch04.ipynb
-  Ch 5: Pretraining on Unlabeled Data	- ch05.ipynb
-  Ch 6: Finetuning for Text Classification	- ch06.ipynb
-  Ch 7: Finetuning to Follow Instructions
-   Understanding reasoning Models	No code
-   Generating Text with a Pre-trained LLM	- ch02_main.ipynb
-   Evaluating Reasoning Models

# ðŸ“š Additional Books & Resources  

## **Books**  
1.  [Hands-On Large Language Models: Language Understanding and Generation](https://www.amazon.in/Hands-Large-Language-Models-Understanding/dp/935542552X/ref=pd_sbs_d_sccl_1_1/521-7549942-9569643?pd_rd_w=Ueibj&content-id=amzn1.sym.6d240404-f8ea-42f5-98fe-bf3c8ec77086&pf_rd_p=6d240404-f8ea-42f5-98fe-bf3c8ec77086&pf_rd_r=Z9BASYAF4RW1MVP0D173&pd_rd_wg=ZUKds&pd_rd_r=95ab3bb8-4c74-458a-8089-fa654d4b720c&pd_rd_i=935542552X&psc=1) 
2.  [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
3.  [LLM-course](https://github.com/mlabonne/llm-course)  
4. **Speech and Language Processing** â€“ Jurafsky & Martin ([Online Draft](https://web.stanford.edu/~jurafsky/slp3/))  
5. **Natural Language Processing with Python** â€“ Steven Bird, Ewan Klein, Edward Loper ([Free Online](https://www.nltk.org/book/))  
6. **Transformers for Natural Language Processing** â€“ Denis Rothman  
7. **Deep Learning for NLP** â€“ Palash Goyal, Sumit Pandey, Karan Jain  
8. **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** â€“ AurÃ©lien GÃ©ron  

---
