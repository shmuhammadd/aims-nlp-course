# Natural Language Processing & Large Language Models  
**Course Length:** 15 Lectures Ã— 2 Hours  
**Resources:**  
- [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)  
- [LLM-course](https://github.com/mlabonne/llm-course)  

---

## ðŸ“‘ Course Syllabus

| Lecture | Title | Topics / Activities | Materials |
|---------|-------|---------------------|-----------|
| **1** | Introduction to NLP and LLMs | What is NLP? Evolution: n-grams â†’ RNNs â†’ Transformers â†’ LLMs. Applications and motivations. Course structure & tools. | [Assignment](#) Â· [Code](#) |
| **2** | Text Processing & Linguistic Foundations | Tokenization, normalization, subword models (BPE, SentencePiece). Morphology, syntax, semantics. Classical reps: BoW, TF-IDF, n-grams. | [Assignment](#) Â· [Code](#) |
| **3** | Word Embeddings & Distributional Semantics | Word2Vec, GloVe, FastText. Distributional hypothesis. Intrinsic vs extrinsic evaluation. Hands-on: embedding visualization. | [Assignment](#) Â· [Code](#) |
| **4** | Neural Sequence Models | RNNs, LSTMs, GRUs. Encoderâ€“decoder architectures. Attention as solution to long dependencies. | [Assignment](#) Â· [Code](#) |
| **5** | The Transformer Model | Self-attention & multi-head attention. Positional encodings. Encoder/decoder designs. Hands-on: mini-transformer from scratch. | [Assignment](#) Â· [Code](#) |
| **6** | Training Large Language Models | Pretraining objectives (LM, MLM, denoising). Datasets & tokenization. Optimizers, scaling laws, hardware. | [Assignment](#) Â· [Code](#) |
| **7** | Fine-Tuning & Adaptation | Supervised fine-tuning. LoRA, QLoRA, adapters. Domain adaptation. Hands-on: fine-tune a small model. | [Assignment](#) Â· [Code](#) |
| **8** | Prompting & Instruction Tuning | Prompt engineering. Zero/few-shot, chain-of-thought prompting. Instruction-tuned models (T5, FLAN, Alpaca). | [Assignment](#) Â· [Code](#) |
| **9** | Alignment & RLHF | Human feedback in training loops. Reward modeling & PPO. Safety and alignment challenges. | [Assignment](#) Â· [Code](#) |
| **10** | Evaluation of LLMs | Intrinsic metrics (perplexity, BLEU, ROUGE). Benchmarks (GLUE, SuperGLUE, MMLU). Human eval. Detecting hallucinations/bias. | [Assignment](#) Â· [Code](#) |
| **11** | Model Efficiency & Compression | Quantization (8-bit, 4-bit), pruning, distillation. Flash attention. Deployment tradeoffs. Hands-on: quantize a model. | [Assignment](#) Â· [Code](#) |
| **12** | Retrieval-Augmented Generation (RAG) | Dense embeddings & vector databases. Hybrid retrieval. Applications: knowledge grounding, chatbots. Hands-on: toy RAG pipeline. | [Assignment](#) Â· [Code](#) |
| **13** | Agents, Tools & Multimodal LLMs | LLM agents (LangChain, HuggingFace). Tool use & APIs. Multimodal LLMs (text+image, text+speech). Hands-on: connect LLM with API. | [Assignment](#) Â· [Code](#) |
| **14** | Deployment, Production & Ethics | Serving models (FastAPI, HF Inference). Scaling & monitoring. Ethics: bias, privacy, misuse, prompt injection. | [Assignment](#) Â· [Code](#) |
| **15** | Future Directions & Student Presentations | Mixture of Experts, model merging, continual learning. Open-source vs proprietary race. Student project demos. Wrap-up. | [Assignment](#) Â· [Code](#) |

---

# ðŸ“š Additional Books & Resources  

## **Books**  
1. **Speech and Language Processing** â€“ Jurafsky & Martin ([Online Draft](https://web.stanford.edu/~jurafsky/slp3/))  
2. **Natural Language Processing with Python** â€“ Steven Bird, Ewan Klein, Edward Loper ([Free Online](https://www.nltk.org/book/))  
3. **Transformers for Natural Language Processing** â€“ Denis Rothman  
4. **Deep Learning for NLP** â€“ Palash Goyal, Sumit Pandey, Karan Jain  
5. **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** â€“ AurÃ©lien GÃ©ron  

---
