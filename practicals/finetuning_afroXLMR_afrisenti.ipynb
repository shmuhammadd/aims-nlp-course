{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP7c5iZYk0Jf"
      },
      "source": [
        "# Lab 2: Fine-tuning AfroXLMR for African Language Sentiment Analysis\n",
        "\n",
        "## Learning Objectives\n",
        "In this lab, you will:\n",
        "1. Load a pre-trained multilingual model (AfroXLMR)\n",
        "2. Understand the fine-tuning process\n",
        "3. Fine-tune AfroXLMR on AfriSenti dataset for sentiment analysis\n",
        "4. Evaluate model performance\n",
        "5. Compare results across different African languages\n",
        "\n",
        "## Background\n",
        "**AfroXLMR** is an African-adapted XLM-RoBERTa model pre-trained on 16 African languages. It's an encoder model (like BERT) that excels at understanding tasks.\n",
        "\n",
        "**AfriSenti** is a sentiment analysis dataset covering 14 African languages with tweets labeled as positive, negative, or neutral.\n",
        "\n",
        "## Why Fine-tuning?\n",
        "Remember from the lecture: Pre-training teaches general language knowledge (like going to school), while fine-tuning specializes the model for a specific task (like medical school)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyhFqZnqk0Jg"
      },
      "source": [
        "## Step 1: Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxOcZWbkk0Jg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch accelerate scikit-learn seaborn -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_3gSAnwk0Jg"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrtvld7lk0Jg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ZRV45Qk0Jg"
      },
      "source": [
        "## Step 3: Choose Your Language\n",
        "\n",
        "AfriSenti supports 14 African languages. Choose one to work with!\n",
        "\n",
        "Available languages:\n",
        "- **amh**: Amharic\n",
        "- **arq**: Algerian Arabic\n",
        "- **hau**: Hausa\n",
        "- **ibo**: Igbo\n",
        "- **kin**: Kinyarwanda\n",
        "- **ary**: Moroccan Arabic\n",
        "- **pcm**: Nigerian Pidgin\n",
        "- **oro**: Oromo\n",
        "- **por**: Portuguese (Mozambican)\n",
        "- **swa**: Swahili\n",
        "- **tir**: Tigrinya\n",
        "- **twi**: Twi\n",
        "- **tso**: Xitsonga\n",
        "- **yor**: Yoruba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yTowsVCk0Jh"
      },
      "outputs": [],
      "source": [
        "LANGUAGE = \"hau\"  # Change this to any language code from the list above\n",
        "\n",
        "# Language names for display\n",
        "LANGUAGE_NAMES = {\n",
        "    \"amh\": \"Amharic\",\n",
        "    \"arq\": \"Algerian Arabic\",\n",
        "    \"hau\": \"Hausa\",\n",
        "    \"ibo\": \"Igbo\",\n",
        "    \"kin\": \"Kinyarwanda\",\n",
        "    \"ary\": \"Moroccan Arabic\",\n",
        "    \"pcm\": \"Nigerian Pidgin\",\n",
        "    \"oro\": \"Oromo\",\n",
        "    \"por\": \"Portuguese (Mozambican)\",\n",
        "    \"swa\": \"Swahili\",\n",
        "    \"tir\": \"Tigrinya\",\n",
        "    \"twi\": \"Twi\",\n",
        "    \"tso\": \"Xitsonga\",\n",
        "    \"yor\": \"Yoruba\"\n",
        "}\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Selected Language: {LANGUAGE_NAMES[LANGUAGE]} ({LANGUAGE})\")\n",
        "print(f\"{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414KivP0k0Jh"
      },
      "source": [
        "## Step 4: Load the AfriSenti Dataset\n",
        "\n",
        "We'll load the dataset for your chosen language from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9O-N4vik0Jh"
      },
      "outputs": [],
      "source": [
        "# Load AfriSenti-Twitter dataset directly from GitHub\n",
        "# Note: HuggingFace no longer supports dataset scripts, so we load directly from the repository\n",
        "print(f\"Loading AfriSenti dataset for {LANGUAGE_NAMES[LANGUAGE]}...\")\n",
        "\n",
        "BASE_URL = f\"https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/{LANGUAGE}\"\n",
        "\n",
        "# Load the data using pandas\n",
        "train_df = pd.read_csv(f\"{BASE_URL}/train.tsv\", sep='\\t')\n",
        "val_df = pd.read_csv(f\"{BASE_URL}/dev.tsv\", sep='\\t')\n",
        "test_df = pd.read_csv(f\"{BASE_URL}/test.tsv\", sep='\\t')\n",
        "\n",
        "label_to_id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label_to_id)\n",
        "val_df[\"label\"]   = val_df[\"label\"].map(label_to_id)\n",
        "test_df[\"label\"]  = test_df[\"label\"].map(label_to_id)\n",
        "\n",
        "print(\"\\nDataset loaded successfully!\")\n",
        "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
        "print(f\"Validation set shape: {val_df.shape}\")\n",
        "print(f\"Test set shape: {test_df.shape}\")\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())\n",
        "\n",
        "# Show the first few examples\n",
        "print(f\"\\nFirst 3 examples from training set:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(3):\n",
        "    row = train_df.iloc[i]\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: {row['tweet']}\")\n",
        "    print(f\"Label ID: {row['label']} ({id_to_label[row['label']]})\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Too4DWk0Jh"
      },
      "source": [
        "## Step 5: Explore the Dataset\n",
        "\n",
        "Let's understand the data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc8sljO_k0Jh"
      },
      "outputs": [],
      "source": [
        "# Dataset statistics\n",
        "print(f\"\\nDataset Statistics for {LANGUAGE_NAMES[LANGUAGE]}:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training samples:   {len(train_df):,}\")\n",
        "print(f\"Validation samples: {len(val_df):,}\")\n",
        "print(f\"Test samples:       {len(test_df):,}\")\n",
        "print(f\"Total samples:      {len(train_df) + len(val_df) + len(test_df):,}\")\n",
        "\n",
        "# Label distribution\n",
        "train_dist = train_df['label'].value_counts().to_dict()\n",
        "valid_dist = val_df['label'].value_counts().to_dict()\n",
        "test_dist = test_df['label'].value_counts().to_dict()\n",
        "\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Split':<12} {'Negative':<12} {'Neutral':<12} {'Positive':<12}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "NEG, NEU, POS = 0, 1, 2\n",
        "\n",
        "print(f\"{'Train':<12} {train_dist.get(NEG, 0):<12} {train_dist.get(NEU, 0):<12} {train_dist.get(POS, 0):<12}\")\n",
        "print(f\"{'Validation':<12} {valid_dist.get(NEG, 0):<12} {valid_dist.get(NEU, 0):<12} {valid_dist.get(POS, 0):<12}\")\n",
        "print(f\"{'Test':<12} {test_dist.get(NEG, 0):<12} {test_dist.get(NEU, 0):<12} {test_dist.get(POS, 0):<12}\")\n",
        "\n",
        "label_names = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "# Visualize distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, (split_name, dist) in enumerate([('Train', train_dist), ('Validation', valid_dist), ('Test', test_dist)]):\n",
        "    labels_list = [label_names[k] for k in sorted(dist.keys())]\n",
        "    values = [dist[k] for k in sorted(dist.keys())]\n",
        "\n",
        "    axes[idx].bar(labels_list, values, color=['#e74c3c', '#95a5a6', '#2ecc71'])\n",
        "    axes[idx].set_title(f'{split_name} Set')\n",
        "    axes[idx].set_ylabel('Count')\n",
        "    axes[idx].set_xlabel('Sentiment')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(values):\n",
        "        axes[idx].text(i, v + max(values)*0.02, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.suptitle(f'Sentiment Distribution - {LANGUAGE_NAMES[LANGUAGE]}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'afrisenti_{LANGUAGE}_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Check for class imbalance\n",
        "total_train = sum(train_dist.values())\n",
        "imbalance_ratio = max(train_dist.values()) / min(train_dist.values())\n",
        "print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
        "if imbalance_ratio > 3:\n",
        "    print(\"âš ï¸  Significant class imbalance detected. Consider using class weights!\")\n",
        "else:\n",
        "    print(\"âœ“ Class distribution is relatively balanced.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZfJ_F02k0Ji"
      },
      "source": [
        "## Step 6: Load Pre-trained AfroXLMR Model\n",
        "\n",
        "AfroXLMR was pre-trained on 16 African languages. We'll fine-tune it for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ8Uzg2Dk0Ji"
      },
      "outputs": [],
      "source": [
        "# Model name on Hugging Face\n",
        "MODEL_NAME = \"Davlan/afro-xlmr-small\"\n",
        "\n",
        "print(f\"Loading AfroXLMR model and tokenizer...\")\n",
        "print(f\"Model: {MODEL_NAME}\\n\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model for sequence classification (sentiment analysis)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=3,  # 3 classes: negative, neutral, positive\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"âœ“ Model loaded successfully!\")\n",
        "print(f\"âœ“ Tokenizer loaded successfully!\")\n",
        "print(f\"\\nModel has {model.num_parameters():,} parameters\")\n",
        "print(f\"Model is on device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEueNalqk0Ji"
      },
      "source": [
        "## Step 7: Tokenize the Dataset\n",
        "\n",
        "We need to convert text into tokens that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEnEGCmHk0Ji"
      },
      "outputs": [],
      "source": [
        "# Convert pandas DataFrames to Hugging Face datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['tweet'],\n",
        "        padding=False,  # We'll pad dynamically in batches\n",
        "        truncation=True,\n",
        "        max_length=128,  # Maximum sequence length\n",
        "    )\n",
        "\n",
        "# Tokenize all splits\n",
        "print(\"Tokenizing datasets...\")\n",
        "print(\"This may take a minute...\\n\")\n",
        "\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['tweet']  # Remove text column, keep only tokens and labels\n",
        ")\n",
        "\n",
        "tokenized_valid = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['tweet']\n",
        ")\n",
        "\n",
        "tokenized_test = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['tweet']\n",
        ")\n",
        "\n",
        "print(\"âœ“ Tokenization complete!\\n\")\n",
        "\n",
        "# Example of tokenization\n",
        "example = train_df.iloc[0]\n",
        "tokens = tokenizer.tokenize(example['tweet'])\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Example of Tokenization:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Original text: {example['tweet']}\")\n",
        "print(f\"\\nTokens: {tokens[:10]}... ({len(tokens)} total)\")\n",
        "print(f\"\\nToken IDs: {token_ids[:10]}... ({len(token_ids)} total)\")\n",
        "print(f\"\\nLabel: {label_names[example['label']]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHCQ8DVwk0Ji"
      },
      "source": [
        "## Step 8: Set Up Data Collator\n",
        "\n",
        "The data collator handles dynamic padding of sequences in each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeZ-SiCOk0Ji"
      },
      "outputs": [],
      "source": [
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "print(\"Data collator created!\")\n",
        "print(\"\\nWhat does the data collator do?\")\n",
        "print(\"- Pads sequences to the same length within each batch\")\n",
        "print(\"- Saves memory by not padding all sequences to max length\")\n",
        "print(\"- Creates attention masks to tell the model which tokens are real vs. padding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mTvOSYGk0Ji"
      },
      "source": [
        "## Step 9: Define Evaluation Metrics\n",
        "\n",
        "We'll compute accuracy, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rsuHbfyk0Ji"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n",
        "\n",
        "print(\"Evaluation metrics defined!\")\n",
        "print(\"\\nMetrics we'll track:\")\n",
        "print(\"- Accuracy: Overall correctness\")\n",
        "print(\"- Precision: How many predicted positives are actually positive\")\n",
        "print(\"- Recall: How many actual positives we correctly identified\")\n",
        "print(\"- F1-score: Harmonic mean of precision and recall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdb55jC_k0Ji"
      },
      "source": [
        "## Step 10: Configure Training Arguments\n",
        "\n",
        "These settings control how the model is fine-tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj6WTF9tk0Ji"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"afroXLMR_{LANGUAGE}_sentiment\",\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=1,              # Number of times to go through the data\n",
        "    per_device_train_batch_size=16,  # Batch size for training\n",
        "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
        "    learning_rate=2e-5,              # Learning rate (small for fine-tuning)\n",
        "    weight_decay=0.01,               # Regularization to prevent overfitting\n",
        "    warmup_ratio=0.1,                # Gradually increase learning rate\n",
        "\n",
        "    # Evaluation and saving\n",
        "    eval_strategy=\"steps\",           # Evaluate during training\n",
        "    eval_steps=200,                  # Evaluate every 100 steps\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,              # Keep only 2 best checkpoints\n",
        "    load_best_model_at_end=True,     # Load best model after training\n",
        "    metric_for_best_model=\"f1\",      # Use F1-score to determine best model\n",
        "\n",
        "    # Logging\n",
        "    logging_dir=f\"logs_{LANGUAGE}\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",                # Don't send to wandb/tensorboard\n",
        "\n",
        "    # Other settings\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision on GPU\n",
        "    dataloader_num_workers=2,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Evaluation strategy: Every {training_args.eval_steps} steps\")\n",
        "print(f\"Mixed precision (FP16): {training_args.fp16}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCFqkdhk0Ji"
      },
      "source": [
        "## Step 11: Create Trainer and Start Fine-tuning\n",
        "\n",
        "Now we'll fine-tune the model! This is where the magic happens. ðŸŽ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJI2hD4jk0Ji"
      },
      "outputs": [],
      "source": [
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Starting Fine-tuning for {LANGUAGE_NAMES[LANGUAGE]} Sentiment Analysis\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "print(\"This will take several minutes. Please be patient...\\n\")\n",
        "\n",
        "# Train the model\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Fine-tuning Complete! ðŸŽ‰\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Print training results\n",
        "print(\"Training Results:\")\n",
        "print(\"-\"*60)\n",
        "print(f\"Training Loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"Training Runtime: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training Samples/Second: {train_result.metrics['train_samples_per_second']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTgtnrvik0Ji"
      },
      "source": [
        "## Step 12: Evaluate on Test Set\n",
        "\n",
        "Let's see how well our fine-tuned model performs on unseen data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4gGO_Klk0Ji"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nEvaluating on {LANGUAGE_NAMES[LANGUAGE]} test set...\\n\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = trainer.evaluate(tokenized_test)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Test Results for {LANGUAGE_NAMES[LANGUAGE]}\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Accuracy:  {test_results['eval_accuracy']:.4f} ({test_results['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Precision: {test_results['eval_precision']:.4f}\")\n",
        "print(f\"Recall:    {test_results['eval_recall']:.4f}\")\n",
        "print(f\"F1-Score:  {test_results['eval_f1']:.4f}\")\n",
        "print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCEapQGk0Ji"
      },
      "source": [
        "## Step 13: Detailed Classification Report\n",
        "\n",
        "Let's get per-class metrics to see how well the model performs on each sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEK_Ga7_k0Ji"
      },
      "outputs": [],
      "source": [
        "# Get predictions on test set\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(\n",
        "    true_labels,\n",
        "    predicted_labels,\n",
        "    target_names=['Negative', 'Neutral', 'Positive'],\n",
        "    digits=4\n",
        ")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(\"=\"*80)\n",
        "print(report)\n",
        "\n",
        "# Save report\n",
        "with open(f'classification_report_{LANGUAGE}.txt', 'w') as f:\n",
        "    f.write(f\"Classification Report for {LANGUAGE_NAMES[LANGUAGE]}\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjfs3m49k0Jj"
      },
      "source": [
        "## Step 14: Confusion Matrix\n",
        "\n",
        "A confusion matrix shows which classes the model confuses with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgTfceDak0Jj"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "    yticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "    cbar_kws={'label': 'Count'}\n",
        ")\n",
        "plt.title(f'Confusion Matrix - {LANGUAGE_NAMES[LANGUAGE]} Sentiment Analysis', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'confusion_matrix_{LANGUAGE}.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix analysis\n",
        "print(\"\\nConfusion Matrix Analysis:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Correctly classified Negative: {cm[0, 0]} out of {cm[0].sum()}\")\n",
        "print(f\"Correctly classified Neutral:  {cm[1, 1]} out of {cm[1].sum()}\")\n",
        "print(f\"Correctly classified Positive: {cm[2, 2]} out of {cm[2].sum()}\")\n",
        "print(\"\\nCommon misclassifications:\")\n",
        "if cm[0, 1] > 5:\n",
        "    print(f\"  - {cm[0, 1]} Negative tweets classified as Neutral\")\n",
        "if cm[0, 2] > 5:\n",
        "    print(f\"  - {cm[0, 2]} Negative tweets classified as Positive\")\n",
        "if cm[2, 1] > 5:\n",
        "    print(f\"  - {cm[2, 1]} Positive tweets classified as Neutral\")\n",
        "if cm[2, 0] > 5:\n",
        "    print(f\"  - {cm[2, 0]} Positive tweets classified as Negative\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwbvWt61k0Jj"
      },
      "source": [
        "## Step 15: Test on Custom Examples\n",
        "\n",
        "Now let's test the model on some custom examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDPNTZaOk0Jj"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"Predict sentiment for a given text\"\"\"\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    label_names = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "\n",
        "    return {\n",
        "        'label': label_names[predicted_class],\n",
        "        'confidence': probabilities[0][predicted_class].item(),\n",
        "        'probabilities': {\n",
        "            'Negative': probabilities[0][0].item(),\n",
        "            'Neutral': probabilities[0][1].item(),\n",
        "            'Positive': probabilities[0][2].item(),\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Example texts (you can modify these)\n",
        "# These are generic examples - replace with language-specific examples!\n",
        "custom_examples = [\n",
        "    \"Wannan abun yayi kyau, ina son shi!\",\n",
        "    \"Wannan abun bai yi kyau ba. Na tsane shi!\",\n",
        "    \"Yau ana ruwa a Cape Town\",\n",
        "]\n",
        "\n",
        "print(f\"\\nTesting Custom Examples for {LANGUAGE_NAMES[LANGUAGE]}:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, example in enumerate(custom_examples, 1):\n",
        "    result = predict_sentiment(example, model, tokenizer)\n",
        "\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Text: {example}\")\n",
        "    print(f\"Predicted: {result['label']} (Confidence: {result['confidence']:.2%})\")\n",
        "    print(f\"Probabilities:\")\n",
        "    for sentiment, prob in result['probabilities'].items():\n",
        "        print(f\"  {sentiment:>8}: {prob:6.2%} {'â–ˆ' * int(prob * 50)}\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg_v4lRfk0Jj"
      },
      "source": [
        "## Step 16: Visualize Training History\n",
        "\n",
        "Let's plot the training and validation metrics over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsd1m6elk0Jj"
      },
      "outputs": [],
      "source": [
        "# Extract training history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Separate training and evaluation logs\n",
        "train_logs = [log for log in log_history if 'loss' in log]\n",
        "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "# Extract metrics\n",
        "train_steps = [log['step'] for log in train_logs]\n",
        "train_losses = [log['loss'] for log in train_logs]\n",
        "\n",
        "eval_steps = [log['step'] for log in eval_logs]\n",
        "eval_losses = [log['eval_loss'] for log in eval_logs]\n",
        "eval_f1 = [log['eval_f1'] for log in eval_logs]\n",
        "eval_accuracy = [log['eval_accuracy'] for log in eval_logs]\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(train_steps, train_losses, label='Training Loss', marker='o', markersize=4)\n",
        "axes[0].plot(eval_steps, eval_losses, label='Validation Loss', marker='s', markersize=4)\n",
        "axes[0].set_xlabel('Steps')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# F1-Score\n",
        "axes[1].plot(eval_steps, eval_f1, label='Validation F1', marker='o', color='green', markersize=4)\n",
        "axes[1].set_xlabel('Steps')\n",
        "axes[1].set_ylabel('F1-Score')\n",
        "axes[1].set_title('Validation F1-Score')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[2].plot(eval_steps, eval_accuracy, label='Validation Accuracy', marker='s', color='purple', markersize=4)\n",
        "axes[2].set_xlabel('Steps')\n",
        "axes[2].set_ylabel('Accuracy')\n",
        "axes[2].set_title('Validation Accuracy')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'Training History - {LANGUAGE_NAMES[LANGUAGE]}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'training_history_{LANGUAGE}.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C936cS8fk0Jk"
      },
      "source": [
        "## Step 17: Save Your Fine-tuned Model\n",
        "\n",
        "Save the model so you can use it later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5HE66yNk0Jk"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "output_dir = f\"afroXLMR_{LANGUAGE}_sentiment\"\n",
        "\n",
        "print(f\"Saving fine-tuned model to {output_dir}...\")\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(\"\\nâœ“ Model saved successfully!\")\n",
        "print(f\"\\nYou can load the model later using:\")\n",
        "print(f\"  model = AutoModelForSequenceClassification.from_pretrained('{output_dir}')\")\n",
        "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{output_dir}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqOQu6qNk0Jk"
      },
      "source": [
        "## Step 18: Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Fine-tuning Process**\n",
        "   - Started with a pre-trained model (AfroXLMR)\n",
        "   - Adapted it to a specific task (sentiment analysis)\n",
        "   - Used a small, task-specific dataset\n",
        "   - Much faster than training from scratch!\n",
        "\n",
        "2. **Transfer Learning Benefits**\n",
        "   - AfroXLMR already knows African languages\n",
        "   - We just taught it to classify sentiments\n",
        "   - Like teaching a multilingual person a new skill!\n",
        "\n",
        "3. **Evaluation Matters**\n",
        "   - Multiple metrics give a complete picture\n",
        "   - Confusion matrix shows where model struggles\n",
        "   - Important to check per-class performance\n",
        "\n",
        "### Connection to Lecture\n",
        "Remember the analogy:\n",
        "- **Pre-training** (AfroXLMR on web data) = Going to school\n",
        "- **Fine-tuning** (Our sentiment task) = Medical school specialization\n",
        "\n",
        "### Model Performance\n",
        "Your model achieved:\n",
        "- **Accuracy**: {:.2%}\n",
        "- **F1-Score**: {:.4f}\n",
        "\n",
        "This shows the power of transfer learning for African languages!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhQ_u0npk0Jk"
      },
      "source": [
        "## Exercise Questions\n",
        "\n",
        "1. **Understanding Fine-tuning**: Why do we use a small learning rate (2e-5) instead of a large one during fine-tuning?\n",
        "\n",
        "2. **Language Comparison**: Try fine-tuning on a different African language. How do the results compare? Why might performance differ?\n",
        "\n",
        "3. **Error Analysis**: Look at the confusion matrix. Which sentiments does the model confuse most? Why?\n",
        "\n",
        "4. **Data Requirements**: We used hundreds of examples. How many examples would you need for good performance? What's the minimum?\n",
        "\n",
        "5. **Real-world Application**: How would you deploy this model for real-time sentiment analysis of social media in your chosen language?\n",
        "\n",
        "6. **Improvement Ideas**: What could you do to improve model performance?\n",
        "   - More training data?\n",
        "   - Different hyperparameters?\n",
        "   - Ensemble methods?\n",
        "   - Data augmentation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBHC5UyZk0Jk"
      },
      "source": [
        "## Optional: Try Different Languages\n",
        "\n",
        "Want to compare performance across languages? Just change the `LANGUAGE` variable in Step 3 and re-run the notebook!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}