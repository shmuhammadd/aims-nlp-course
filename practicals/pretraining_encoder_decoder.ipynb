{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayMj3edckwvi"
      },
      "source": [
        "# Lab 1: Pre-training Encoder and Decoder Models from Scratch\n",
        "\n",
        "## Learning Objectives\n",
        "In this lab, you will:\n",
        "1. Understand the difference between encoder and decoder architectures\n",
        "2. Implement Masked Language Modeling (MLM) for encoder pre-training\n",
        "3. Implement Causal Language Modeling (CLM) for decoder pre-training\n",
        "4. Train both models on a toy dataset\n",
        "5. Compare their strengths and limitations\n",
        "\n",
        "## Background\n",
        "From the lecture, you learned that:\n",
        "- **Encoders** (like BERT) use bidirectional context and are pre-trained with Masked Language Modeling\n",
        "- **Decoders** (like GPT) use unidirectional context and are pre-trained with Causal Language Modeling\n",
        "- Pre-training teaches models general language understanding before fine-tuning on specific tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvYLCROekwvk"
      },
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "We'll use the Hugging Face `transformers` library and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG7rKYX0kwvl"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets torch tokenizers accelerate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufbj5CSAkwvl"
      },
      "source": [
        "## Step 2: Import Libraries and Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RIWgUN8kwvm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    BertConfig, BertForMaskedLM,\n",
        "    GPT2Config, GPT2LMHeadModel,\n",
        "    PreTrainedTokenizerFast,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532vKa11kwvm"
      },
      "source": [
        "## Step 3: Create a Toy Dataset\n",
        "\n",
        "We'll create a small corpus about African animals and culture. This toy dataset will be used to pre-train both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah0f9QrTkwvm"
      },
      "outputs": [],
      "source": [
        "# Toy corpus - sentences about African themes\n",
        "toy_corpus = [\n",
        "    \"The lion is the king of the savanna.\",\n",
        "    \"Elephants are the largest land animals in Africa.\",\n",
        "    \"Giraffes have very long necks to reach tall trees.\",\n",
        "    \"Zebras have black and white stripes.\",\n",
        "    \"The cheetah is the fastest land animal.\",\n",
        "    \"Hippos spend most of their time in water.\",\n",
        "    \"Rhinos have thick skin and large horns.\",\n",
        "    \"Lions live in groups called prides.\",\n",
        "    \"Elephants use their trunks to drink water.\",\n",
        "    \"The leopard is a skilled climber.\",\n",
        "    \"African cultures are rich and diverse.\",\n",
        "    \"Many languages are spoken across the continent.\",\n",
        "    \"Traditional music uses drums and dancing.\",\n",
        "    \"The baobab tree is called the tree of life.\",\n",
        "    \"The Sahara is the largest hot desert.\",\n",
        "    \"Victoria Falls is one of the largest waterfalls.\",\n",
        "    \"Mount Kilimanjaro is the highest mountain in Africa.\",\n",
        "    \"The Nile is the longest river in the world.\",\n",
        "    \"Coral reefs exist along the coast.\",\n",
        "    \"Rainforests are home to many species.\",\n",
        "    \"The sun shines brightly in the sky.\",\n",
        "    \"Birds fly from tree to tree.\",\n",
        "    \"Fish swim in the rivers and lakes.\",\n",
        "    \"People farm crops like maize and cassava.\",\n",
        "    \"Children play games in the village.\",\n",
        "    \"Markets sell fruits and vegetables.\",\n",
        "    \"Storytelling is an important tradition.\",\n",
        "    \"Elders share wisdom with the young.\",\n",
        "    \"Music and dance celebrate life.\",\n",
        "    \"Artists create beautiful sculptures and paintings.\",\n",
        "]\n",
        "\n",
        "# Duplicate the corpus to have more training data\n",
        "toy_corpus = toy_corpus * 20  # Now we have 600 sentences\n",
        "\n",
        "print(f\"Total sentences in corpus: {len(toy_corpus)}\")\n",
        "print(\"\\nFirst 5 sentences:\")\n",
        "for i, sent in enumerate(toy_corpus[:5]):\n",
        "    print(f\"{i+1}. {sent}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXPZ17Nckwvm"
      },
      "source": [
        "## Step 4: Train a Tokenizer\n",
        "\n",
        "Both models need a tokenizer to convert text into tokens. We'll train a simple WordPiece tokenizer from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5MeavKJkwvm"
      },
      "outputs": [],
      "source": [
        "# Create a WordPiece tokenizer\n",
        "tokenizer_model = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer_model.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Train the tokenizer\n",
        "trainer = trainers.WordPieceTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "# Save corpus to a temporary file for training\n",
        "with open(\"toy_corpus.txt\", \"w\") as f:\n",
        "    for text in toy_corpus:\n",
        "        f.write(text + \"\\n\")\n",
        "\n",
        "tokenizer_model.train(files=[\"toy_corpus.txt\"], trainer=trainer)\n",
        "\n",
        "# Save and load as PreTrainedTokenizerFast\n",
        "tokenizer_model.save(\"toy_tokenizer.json\")\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"toy_tokenizer.json\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")\n",
        "\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"\\nExample tokenization:\")\n",
        "example = \"The lion is the king of the savanna.\"\n",
        "tokens = tokenizer.tokenize(example)\n",
        "print(f\"Text: {example}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ud1Dulkwvn"
      },
      "source": [
        "## Step 5: Prepare Dataset for Training\n",
        "\n",
        "We'll tokenize our corpus and create a Hugging Face Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxgKGmOokwvn"
      },
      "outputs": [],
      "source": [
        "# Tokenize the corpus\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=64, padding=\"max_length\")\n",
        "\n",
        "# Create dataset\n",
        "dataset_dict = {\"text\": toy_corpus}\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Tokenize\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "# Split into train and validation\n",
        "train_size = int(0.9 * len(tokenized_dataset))\n",
        "eval_size = len(tokenized_dataset) - train_size\n",
        "\n",
        "train_dataset = tokenized_dataset.select(range(train_size))\n",
        "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7NHQ4xkwvn"
      },
      "source": [
        "## Step 6: Pre-train an Encoder Model (BERT-style)\n",
        "\n",
        "### 6.1 Understanding Masked Language Modeling (MLM)\n",
        "\n",
        "MLM works by:\n",
        "1. Randomly masking 15% of tokens in each sentence\n",
        "2. Training the model to predict the masked tokens\n",
        "3. Using bidirectional context (looking at words before AND after the mask)\n",
        "\n",
        "Example:\n",
        "- Original: \"The lion is the king of the savanna\"\n",
        "- Masked: \"The lion is the [MASK] of the savanna\"\n",
        "- Model predicts: \"king\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p72D40cvkwvn"
      },
      "outputs": [],
      "source": [
        "# Create a small BERT model configuration\n",
        "\n",
        "encoder_config = BertConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    hidden_size=128,           # Small for our toy dataset\n",
        "    num_hidden_layers=2,       # Only 2 layers\n",
        "    num_attention_heads=2,     # 2 attention heads\n",
        "    intermediate_size=512,\n",
        "    max_position_embeddings=64,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "encoder_model = BertForMaskedLM(encoder_config)\n",
        "\n",
        "print(f\"Encoder model parameters: {encoder_model.num_parameters():,}\")\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(encoder_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx7V1eHtkwvn"
      },
      "source": [
        "### 6.2 Set Up MLM Data Collator\n",
        "\n",
        "The data collator automatically masks tokens for MLM training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZZEyeALkwvn"
      },
      "outputs": [],
      "source": [
        "# Data collator for MLM (automatically masks tokens)\n",
        "mlm_data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,              # Enable Masked Language Modeling\n",
        "    mlm_probability=0.15   # Mask 15% of tokens (as in BERT paper)\n",
        ")\n",
        "\n",
        "# Let's see an example of how masking works\n",
        "print(\"Example of MLM masking:\")\n",
        "print(\"-\" * 50)\n",
        "example_text = \"The lion is the king of the savanna.\"\n",
        "inputs = tokenizer(example_text, return_tensors=\"pt\", padding=\"max_length\", max_length=64)\n",
        "\n",
        "# Apply data collator to see masking\n",
        "# Note: data collator expects a list of dictionaries, not tensors\n",
        "batch = [{key: val[0] for key, val in inputs.items()}]\n",
        "masked_batch = mlm_data_collator(batch)\n",
        "\n",
        "print(f\"Original text: {example_text}\")\n",
        "print(f\"\\nOriginal tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist()[:15])}\")\n",
        "print(f\"\\nMasked tokens: {tokenizer.convert_ids_to_tokens(masked_batch['input_ids'][0].tolist()[:15])}\")\n",
        "print(f\"\\n'[MASK]' represents tokens the model must predict!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4uqzkzTkwvn"
      },
      "source": [
        "### 6.3 Train the Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhGaND0vkwvn"
      },
      "outputs": [],
      "source": [
        "# Training arguments for encoder\n",
        "encoder_training_args = TrainingArguments(\n",
        "    output_dir=\"bert_toy\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "encoder_trainer = Trainer(\n",
        "    model=encoder_model,\n",
        "    args=encoder_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=mlm_data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting encoder pre-training with Masked Language Modeling...\")\n",
        "print(\"This will take a few minutes.\\n\")\n",
        "encoder_results = encoder_trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Encoder Pre-training Complete!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt1A0mo_kwvo"
      },
      "source": [
        "### 6.4 Test the Encoder Model\n",
        "\n",
        "Let's test if the model learned to predict masked words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2RUt4KMkwvo"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a fill-mask pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=encoder_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Test sentences with [MASK]\n",
        "test_sentences = [\n",
        "    \"The lion is the [MASK] of the savanna.\",\n",
        "    \"Elephants are the [MASK] land animals.\",\n",
        "    \"The cheetah is the [MASK] land animal.\",\n",
        "]\n",
        "\n",
        "print(\"Testing Encoder Model (MLM):\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for sent in test_sentences:\n",
        "    print(f\"\\nSentence: {sent}\")\n",
        "    results = fill_mask(sent, top_k=3)\n",
        "    print(\"Predictions:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"  {i}. {result['token_str']:>10} (score: {result['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZiptAgkwvo"
      },
      "source": [
        "## Step 7: Pre-train a Decoder Model (GPT-style)\n",
        "\n",
        "### 7.1 Understanding Causal Language Modeling (CLM)\n",
        "\n",
        "CLM works by:\n",
        "1. Predicting the next word based ONLY on previous words\n",
        "2. Using unidirectional (left-to-right) context\n",
        "3. Training the model to continue text naturally\n",
        "\n",
        "Example:\n",
        "- Input: \"The lion is the\"\n",
        "- Model predicts: \"king\"\n",
        "- Then: \"The lion is the king\"\n",
        "- Model predicts: \"of\"\n",
        "- And so on..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm7VmTO-kwvo"
      },
      "outputs": [],
      "source": [
        "# Create a small GPT-2 model configuration\n",
        "decoder_config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_positions=64,\n",
        "    n_embd=128,\n",
        "    n_layer=2,\n",
        "    n_head=2,\n",
        "    bos_token_id=tokenizer.cls_token_id,\n",
        "    eos_token_id=tokenizer.sep_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "decoder_model = GPT2LMHeadModel(decoder_config)\n",
        "\n",
        "print(f\"Decoder model parameters: {decoder_model.num_parameters():,}\")\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(decoder_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB1htPvLkwvo"
      },
      "source": [
        "### 7.2 Set Up CLM Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzGiaOY6kwvo"
      },
      "outputs": [],
      "source": [
        "# Data collator for CLM (no masking, just shift labels)\n",
        "clm_data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Disable MLM for causal language modeling\n",
        ")\n",
        "\n",
        "print(\"CLM Training Setup:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"In CLM, the model learns to predict the next token.\")\n",
        "print(\"\\nExample:\")\n",
        "print(\"Input:  The lion is the king\")\n",
        "print(\"Target: lion is the king of\")\n",
        "print(\"\\nThe model learns: given 'The', predict 'lion'\")\n",
        "print(\"                   given 'The lion', predict 'is'\")\n",
        "print(\"                   given 'The lion is', predict 'the'\")\n",
        "print(\"                   ... and so on.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNo-B9fkwvo"
      },
      "source": [
        "### 7.3 Train the Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co5TLSmnkwvp"
      },
      "outputs": [],
      "source": [
        "# Training arguments for decoder\n",
        "decoder_training_args = TrainingArguments(\n",
        "    output_dir=\"gpt_toy\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "decoder_trainer = Trainer(\n",
        "    model=decoder_model,\n",
        "    args=decoder_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=clm_data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting decoder pre-training with Causal Language Modeling...\")\n",
        "print(\"This will take a few minutes.\\n\")\n",
        "decoder_results = decoder_trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Decoder Pre-training Complete!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzb3hYTvkwvp"
      },
      "source": [
        "### 7.4 Test the Decoder Model\n",
        "\n",
        "Let's test if the model learned to generate text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
        "print(\"Model embed size:\", decoder_model.get_input_embeddings().weight.size(0))\n"
      ],
      "metadata": {
        "id": "FBCLMZ4RoO5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uROLtcJ5kwvp"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=decoder_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The lion is\",\n",
        "    \"Elephants are\",\n",
        "    \"The cheetah is the\",\n",
        "]\n",
        "\n",
        "print(\"Testing Decoder Model (Text Generation):\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    outputs = text_generator(\n",
        "        prompt,\n",
        "        max_new_tokens=30,          # <-- use this instead of max_length\n",
        "        truncation=True,            # <-- required to avoid overflow\n",
        "        num_return_sequences=2,\n",
        "        temperature=0.8,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "    )\n",
        "    print(\"Generated texts:\")\n",
        "    for i, output in enumerate(outputs, 1):\n",
        "        print(f\"  {i}. {output['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FYaK3p6kwvp"
      },
      "source": [
        "## Step 8: Compare Encoder vs Decoder\n",
        "\n",
        "Let's visualize and compare the training losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWcMitjHkwvp"
      },
      "outputs": [],
      "source": [
        "# Extract training histories\n",
        "encoder_history = encoder_trainer.state.log_history\n",
        "decoder_history = decoder_trainer.state.log_history\n",
        "\n",
        "# Get training losses\n",
        "encoder_train_loss = [log['loss'] for log in encoder_history if 'loss' in log]\n",
        "decoder_train_loss = [log['loss'] for log in decoder_history if 'loss' in log]\n",
        "\n",
        "# Get eval losses\n",
        "encoder_eval_loss = [log['eval_loss'] for log in encoder_history if 'eval_loss' in log]\n",
        "decoder_eval_loss = [log['eval_loss'] for log in decoder_history if 'eval_loss' in log]\n",
        "\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Training loss\n",
        "axes[0].plot(encoder_train_loss, label='Encoder (BERT)', marker='o')\n",
        "axes[0].plot(decoder_train_loss, label='Decoder (GPT)', marker='s')\n",
        "axes[0].set_xlabel('Steps')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss Comparison')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation loss\n",
        "axes[1].plot(encoder_eval_loss, label='Encoder (BERT)', marker='o')\n",
        "axes[1].plot(decoder_eval_loss, label='Decoder (GPT)', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Validation Loss Comparison')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Encoder final train loss: {encoder_train_loss[-1]:.4f}\")\n",
        "print(f\"Encoder final eval loss:  {encoder_eval_loss[-1]:.4f}\")\n",
        "print(f\"\\nDecoder final train loss: {decoder_train_loss[-1]:.4f}\")\n",
        "print(f\"Decoder final eval loss:  {decoder_eval_loss[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO3bMvZwkwvp"
      },
      "source": [
        "## Step 9: Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Encoder Models (BERT-style)**\n",
        "   - Use **Masked Language Modeling (MLM)** for pre-training\n",
        "   - Look at **bidirectional context** (words before AND after)\n",
        "   - Good for: understanding tasks, classification, NER, QA\n",
        "   - Limitation: Cannot naturally generate long text\n",
        "\n",
        "2. **Decoder Models (GPT-style)**\n",
        "   - Use **Causal Language Modeling (CLM)** for pre-training\n",
        "   - Look at **unidirectional context** (only previous words)\n",
        "   - Good for: text generation, completion, creative writing\n",
        "   - Limitation: Cannot see future context\n",
        "\n",
        "3. **Pre-training Importance**\n",
        "   - Teaches models general language patterns\n",
        "   - Transfer learning: \"pre-train once, fine-tune many times\"\n",
        "   - Real models use much larger datasets (billions of words)\n",
        "\n",
        "### Analogy from Lecture\n",
        "Remember: Pre-training is like going to school (learning general knowledge), and fine-tuning is like medical school (specializing)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHFeU7jhkwvp"
      },
      "source": [
        "## Exercise Questions\n",
        "\n",
        "1. **Understanding MLM**: Why do we mask 15% of tokens instead of 50% or 5%? What would happen with extreme values?\n",
        "\n",
        "2. **Bidirectional vs Unidirectional**: Complete this sentence using both models: \"The elephant uses its ___\". Which model gives better predictions and why?\n",
        "\n",
        "3. **Generation Quality**: Try generating text starting with \"The\" using the decoder. Why might the quality be limited?\n",
        "\n",
        "4. **Dataset Size**: We used only 600 sentences. How would results change with 1 million sentences?\n",
        "\n",
        "5. **Architecture Choice**: For each task below, would you use an encoder or decoder?\n",
        "   - Sentiment analysis\n",
        "   - Story completion\n",
        "   - Named entity recognition\n",
        "   - Chatbot\n",
        "   - Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNJBxGbbkwvp"
      },
      "source": [
        "## Optional: Save Your Models\n",
        "\n",
        "You can save the pre-trained models for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZCqOObGkwvq"
      },
      "outputs": [],
      "source": [
        "# Save encoder model\n",
        "encoder_model.save_pretrained(\"pretrained_models/toy_bert\")\n",
        "tokenizer.save_pretrained(\"pretrained_models/toy_bert\")\n",
        "\n",
        "# Save decoder model\n",
        "decoder_model.save_pretrained(\"pretrained_models/toy_gpt\")\n",
        "tokenizer.save_pretrained(\"pretrained_models/toy_gpt\")\n",
        "\n",
        "print(\"Models saved successfully!\")\n",
        "print(\"You can load them later using:\")\n",
        "print(\"  BertForMaskedLM.from_pretrained('pretrained_models/toy_bert')\")\n",
        "print(\"  GPT2LMHeadModel.from_pretrained('pretrained_models/toy_gpt')\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}