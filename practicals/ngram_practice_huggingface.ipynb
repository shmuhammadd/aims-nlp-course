{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models - Practice with Real Data\n",
    "\n",
    "**Course:** Natural Language Processing  \n",
    "**Topic:** Building N-gram Models with HuggingFace Datasets  \n",
    "\n",
    "In this notebook, you will:\n",
    "1. Load real text corpora from HuggingFace\n",
    "2. Implement n-gram language models from scratch\n",
    "3. Train and evaluate your models\n",
    "4. Generate text and compare different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets -q\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Dataset from HuggingFace\n",
    "\n",
    "We'll use the **WikiText-2** dataset, which contains good quality text from Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText-2 dataset\n",
    "print(\"Loading WikiText-2 dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Explore the data\n",
    "print(\"\\nSample from training set:\")\n",
    "print(dataset['train'][0])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(dataset['train'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Data Exploration\n",
    "\n",
    "Answer these questions:\n",
    "1. How many examples are in train/validation/test splits?\n",
    "2. What does each example look like?\n",
    "3. How many empty or very short texts are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Count examples in each split\n",
    "train_size = len(dataset['train'])\n",
    "valid_size = len(dataset['validation'])\n",
    "test_size = len(dataset['test'])\n",
    "\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Validation size: {valid_size}\")\n",
    "print(f\"Test size: {test_size}\")\n",
    "\n",
    "# Question 2: Look at a few examples\n",
    "print(\"\\nFirst 5 examples from training set:\")\n",
    "# TODO: YOUR CODE HERE - print first 5 examples\n",
    "\n",
    "\n",
    "# Question 3: Check for empty/short texts\n",
    "print(\"\\nChecking for empty or short texts:\")\n",
    "# TODO: YOUR CODE HERE - count texts with length 0 or < 5 words\n",
    "# Hint: Use len(text['text'].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Implement Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, add_start_end: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess text:\n",
    "    - Convert to lowercase\n",
    "    - Split into tokens (simple whitespace tokenization)\n",
    "    - Add <s> and </s> tokens if requested\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        add_start_end: Whether to add start/end tokens\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Step 1: Convert to lowercase\n",
    "    \n",
    "    # Step 2: Split on whitespace\n",
    "    \n",
    "    # Step 3: Add <s> and </s> if requested\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "test_text = \"Hello World! This is a test.\"\n",
    "tokens = preprocess_text(test_text, add_start_end=True)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Expected: ['<s>', 'hello', 'world!', 'this', 'is', 'a', 'test.', '</s>']\")\n",
    "print(f\"Your output: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Prepare Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(dataset_split, max_examples: int = None) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Prepare corpus from dataset split.\n",
    "    \n",
    "    Args:\n",
    "        dataset_split: HuggingFace dataset split\n",
    "        max_examples: Maximum number of examples to use (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        List of tokenized sentences\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # 1. Iterate through the dataset\n",
    "    # 2. Skip empty texts (text['text'].strip() == '')\n",
    "    # 3. Preprocess each text using your function above\n",
    "    # 4. Add to corpus only if result has more than 2 tokens\n",
    "    # 5. Stop at max_examples if specified\n",
    "    \n",
    "    # Hint: Use this structure:\n",
    "    # count = 0\n",
    "    # for item in tqdm(dataset_split, desc=\"Processing\"):\n",
    "    #     text = item['text'].strip()\n",
    "    #     if not text:\n",
    "    #         continue\n",
    "    #     ...\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Prepare training corpus (use subset for speed)\n",
    "print(\"Preparing training corpus...\")\n",
    "train_corpus = prepare_corpus(dataset['train'], max_examples=1000)\n",
    "\n",
    "print(f\"\\nCorpus size: {len(train_corpus)} sentences\")\n",
    "print(f\"First sentence: {train_corpus[0][:10]}...\")  # Show first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(corpus: List[List[str]]) -> set:\n",
    "    \"\"\"\n",
    "    Build vocabulary from corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "        Set of unique tokens\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Hint: Create a set and add all tokens from all sentences\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocabulary(train_corpus)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample words: {list(vocab)[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Implement N-gram Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Count Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(corpus: List[List[str]]) -> Counter:\n",
    "    \"\"\"\n",
    "    Count unigram frequencies.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "        Counter with unigram counts\n",
    "    \"\"\"\n",
    "    unigram_counts = Counter()\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # Iterate through corpus and count each token\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "unigram_counts = count_unigrams(train_corpus)\n",
    "print(f\"Total unigrams: {sum(unigram_counts.values())}\")\n",
    "print(f\"Unique unigrams: {len(unigram_counts)}\")\n",
    "print(f\"\\nMost common unigrams:\")\n",
    "print(unigram_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Count Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(corpus: List[List[str]]) -> Counter:\n",
    "    \"\"\"\n",
    "    Count bigram frequencies.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "        Counter with bigram counts (bigrams as tuples)\n",
    "    \"\"\"\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # For each sentence, create pairs of consecutive words\n",
    "    # Store as tuples: (word1, word2)\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "bigram_counts = count_bigrams(train_corpus)\n",
    "print(f\"Total bigrams: {sum(bigram_counts.values())}\")\n",
    "print(f\"Unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"\\nMost common bigrams:\")\n",
    "print(bigram_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Count Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trigrams(corpus: List[List[str]]) -> Counter:\n",
    "    \"\"\"\n",
    "    Count trigram frequencies.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "        Counter with trigram counts (trigrams as tuples)\n",
    "    \"\"\"\n",
    "    trigram_counts = Counter()\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    # For each sentence, create triples of consecutive words\n",
    "    # Store as tuples: (word1, word2, word3)\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "trigram_counts = count_trigrams(train_corpus)\n",
    "print(f\"Total trigrams: {sum(trigram_counts.values())}\")\n",
    "print(f\"Unique trigrams: {len(trigram_counts)}\")\n",
    "print(f\"\\nMost common trigrams:\")\n",
    "print(trigram_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \"\"\"Unigram language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[List[str]]):\n",
    "        self.unigram_counts = count_unigrams(corpus)\n",
    "        self.total_words = sum(self.unigram_counts.values())\n",
    "    \n",
    "    def probability(self, word: str) -> float:\n",
    "        \"\"\"Calculate P(word).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # P(word) = C(word) / N\n",
    "        pass\n",
    "    \n",
    "    def log_probability(self, word: str) -> float:\n",
    "        \"\"\"Calculate log P(word).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # Handle zero probability by returning float('-inf')\n",
    "        pass\n",
    "    \n",
    "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"Calculate log P(sentence).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # Sum of log probabilities of all words\n",
    "        pass\n",
    "\n",
    "# Test your model\n",
    "unigram_model = UnigramModel(train_corpus)\n",
    "test_words = [\"the\", \"of\", \"and\", \"<s>\", \"</s>\"]\n",
    "print(\"Unigram Probabilities:\")\n",
    "for word in test_words:\n",
    "    prob = unigram_model.probability(word)\n",
    "    print(f\"  P({word}) = {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    \"\"\"Bigram language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[List[str]]):\n",
    "        self.unigram_counts = count_unigrams(corpus)\n",
    "        self.bigram_counts = count_bigrams(corpus)\n",
    "    \n",
    "    def probability(self, word: str, previous_word: str) -> float:\n",
    "        \"\"\"Calculate P(word | previous_word).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # P(wi | wi-1) = C(wi-1, wi) / C(wi-1)\n",
    "        pass\n",
    "    \n",
    "    def log_probability(self, word: str, previous_word: str) -> float:\n",
    "        \"\"\"Calculate log P(word | previous_word).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "    \n",
    "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"Calculate log P(sentence).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # Sum log probabilities: log P(w2|w1) + log P(w3|w2) + ...\n",
    "        pass\n",
    "\n",
    "# Test your model\n",
    "bigram_model = BigramModel(train_corpus)\n",
    "test_bigrams = [(\"<s>\", \"the\"), (\"the\", \"and\"), (\"of\", \"the\")]\n",
    "print(\"Bigram Probabilities:\")\n",
    "for w1, w2 in test_bigrams:\n",
    "    prob = bigram_model.probability(w2, w1)\n",
    "    print(f\"  P({w2} | {w1}) = {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel:\n",
    "    \"\"\"Trigram language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[List[str]]):\n",
    "        self.bigram_counts = count_bigrams(corpus)\n",
    "        self.trigram_counts = count_trigrams(corpus)\n",
    "    \n",
    "    def probability(self, word: str, prev_word1: str, prev_word2: str) -> float:\n",
    "        \"\"\"Calculate P(word | prev_word2, prev_word1).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # P(wi | wi-2, wi-1) = C(wi-2, wi-1, wi) / C(wi-2, wi-1)\n",
    "        pass\n",
    "    \n",
    "    def log_probability(self, word: str, prev_word1: str, prev_word2: str) -> float:\n",
    "        \"\"\"Calculate log P(word | prev_word2, prev_word1).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "    \n",
    "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"Calculate log P(sentence).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "\n",
    "# Test your model\n",
    "trigram_model = TrigramModel(train_corpus)\n",
    "if len(train_corpus[0]) >= 3:\n",
    "    w1, w2, w3 = train_corpus[0][:3]\n",
    "    prob = trigram_model.probability(w3, w2, w1)\n",
    "    print(f\"P({w3} | {w1}, {w2}) = {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Evaluation - Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Implement Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, test_corpus: List[List[str]], model_type: str = \"bigram\") -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a language model.\n",
    "    \n",
    "    Perplexity = exp(-1/N * sum(log P(wi | context)))\n",
    "    \n",
    "    Args:\n",
    "        model: Language model with sentence_log_probability method\n",
    "        test_corpus: List of test sentences\n",
    "        model_type: 'unigram', 'bigram', or 'trigram'\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Calculate total log probability across all sentences\n",
    "    # 2. Count total words (excluding <s> for bigram/trigram)\n",
    "    # 3. Calculate average log probability\n",
    "    # 4. Return exp(-avg_log_prob)\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Prepare test corpus\n",
    "print(\"Preparing test corpus...\")\n",
    "test_corpus = prepare_corpus(dataset['validation'], max_examples=100)\n",
    "print(f\"Test corpus size: {len(test_corpus)} sentences\")\n",
    "\n",
    "# Calculate perplexities\n",
    "print(\"\\nCalculating perplexities...\")\n",
    "# TODO: Calculate and print perplexities for all three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1: Implement Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramGenerator:\n",
    "    \"\"\"Generate text using bigram model.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[List[str]]):\n",
    "        self.bigram_model = BigramModel(corpus)\n",
    "        self.unigram_counts = count_unigrams(corpus)\n",
    "        self.bigram_counts = count_bigrams(corpus)\n",
    "        self.next_word_probs = self._build_next_word_distribution()\n",
    "    \n",
    "    def _build_next_word_distribution(self) -> Dict:\n",
    "        \"\"\"Build distribution over next words for each word.\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # For each word w1, build dict of {w2: P(w2|w1)}\n",
    "        pass\n",
    "    \n",
    "    def generate(self, max_length: int = 20) -> List[str]:\n",
    "        \"\"\"Generate a sentence.\"\"\"\n",
    "        # TODO: Implement this\n",
    "        # 1. Start with '<s>'\n",
    "        # 2. Sample next word based on probabilities\n",
    "        # 3. Stop at '</s>' or max_length\n",
    "        # Hint: Use np.random.choice(words, p=probs)\n",
    "        pass\n",
    "\n",
    "# Test generator\n",
    "generator = BigramGenerator(train_corpus)\n",
    "print(\"Generated Sentences:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(10):\n",
    "    sentence = generator.generate(max_length=20)\n",
    "    print(f\"{i+1}. {' '.join(sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1: Implement Add-One Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModelSmoothed:\n",
    "    \"\"\"Bigram model with add-one smoothing.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[List[str]]):\n",
    "        self.unigram_counts = count_unigrams(corpus)\n",
    "        self.bigram_counts = count_bigrams(corpus)\n",
    "        self.vocab = build_vocabulary(corpus)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def probability(self, word: str, previous_word: str) -> float:\n",
    "        \"\"\"Calculate P(word | previous_word) with add-one smoothing.\n",
    "        \n",
    "        Formula: P(wi | wi-1) = (C(wi-1, wi) + 1) / (C(wi-1) + V)\n",
    "        \"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "    \n",
    "    def log_probability(self, word: str, previous_word: str) -> float:\n",
    "        \"\"\"Calculate log P(word | previous_word).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "    \n",
    "    def sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"Calculate log P(sentence).\"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "\n",
    "# Compare with unsmoothed model\n",
    "smoothed_model = BigramModelSmoothed(train_corpus)\n",
    "\n",
    "# Test on unseen bigrams\n",
    "test_unseen = [(\"<s>\", \"quantum\"), (\"the\", \"xylophone\"), (\"amazing\", \"unicorn\")]\n",
    "print(\"Comparison: Unsmoothed vs Smoothed\")\n",
    "print(\"=\" * 60)\n",
    "for w1, w2 in test_unseen:\n",
    "    unsmoothed = bigram_model.probability(w2, w1)\n",
    "    smoothed = smoothed_model.probability(w2, w1)\n",
    "    print(f\"P({w2} | {w1}):\")\n",
    "    print(f\"  Unsmoothed: {unsmoothed:.8f}\")\n",
    "    print(f\"  Smoothed:   {smoothed:.8f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Analysis and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.1: Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a comparison table showing:\n",
    "# 1. Number of parameters (unique n-grams)\n",
    "# 2. Training perplexity\n",
    "# 3. Test perplexity\n",
    "# 4. Sample generated sentences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['Unigram', 'Bigram', 'Trigram', 'Bigram (Smoothed)'],\n",
    "    'Parameters': [\n",
    "        len(unigram_counts),\n",
    "        len(bigram_counts),\n",
    "        len(trigram_counts),\n",
    "        len(bigram_counts)\n",
    "    ],\n",
    "    'Train Perplexity': [\n",
    "        # TODO: Fill in\n",
    "        0.0, 0.0, 0.0, 0.0\n",
    "    ],\n",
    "    'Test Perplexity': [\n",
    "        # TODO: Fill in\n",
    "        0.0, 0.0, 0.0, 0.0\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.2: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualizations:\n",
    "# 1. Bar chart of perplexities\n",
    "# 2. Top-20 most common words\n",
    "# 3. Top-20 most common bigrams\n",
    "\n",
    "# Example for most common words:\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Most common unigrams\n",
    "# TODO: YOUR CODE HERE\n",
    "\n",
    "# Plot 2: Most common bigrams\n",
    "# TODO: YOUR CODE HERE\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Bonus Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Try Different Datasets\n",
    "\n",
    "Load a different dataset from HuggingFace and train your models on it.\n",
    "\n",
    "Suggestions:\n",
    "- `\"wikitext\"` (different versions)\n",
    "- `\"bookcorpus\"`\n",
    "- `\"ptb_text_only\"` (Penn Treebank)\n",
    "- `\"imdb\"` (movie reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE\n",
    "# Try loading and training on a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Implement Better Smoothing\n",
    "\n",
    "Implement add-k smoothing (where k < 1) or interpolation smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Build a Simple Autocomplete System\n",
    "\n",
    "Given a partial sentence, suggest the top-k most likely next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(partial_sentence: str, model: BigramModel, k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Suggest next words for autocomplete.\n",
    "    \n",
    "    Args:\n",
    "        partial_sentence: Incomplete sentence\n",
    "        model: Trained bigram model\n",
    "        k: Number of suggestions\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, probability) tuples\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_partial = \"the cat sat on the\"\n",
    "suggestions = autocomplete(test_partial, bigram_model, k=5)\n",
    "print(f\"Suggestions for: '{test_partial}'\")\n",
    "for word, prob in suggestions:\n",
    "    print(f\"  {word}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
