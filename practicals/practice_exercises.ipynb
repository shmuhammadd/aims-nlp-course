{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Practice Exercises - Pre-training and Fine-tuning\n",
    "\n",
    "## Overview\n",
    "This notebook contains practice exercises to reinforce your understanding of pre-training strategies and fine-tuning. Work through these exercises at your own pace. Solutions are provided at the end.\n",
    "\n",
    "## Learning Goals\n",
    "- Apply pre-training concepts to new scenarios\n",
    "- Experiment with different model architectures\n",
    "- Fine-tune models for various NLP tasks\n",
    "- Analyze and improve model performance\n",
    "- Compare encoder vs decoder models\n",
    "\n",
    "## Time Required\n",
    "Approximately 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch accelerate scikit-learn seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 1: Understanding Masked Language Modeling (MLM)\n",
    "\n",
    "## Objective\n",
    "Understand how MLM masking works and its effect on model training.\n",
    "\n",
    "## Task\n",
    "1. Create a simple dataset with 5 sentences about African wildlife\n",
    "2. Manually implement a masking function that masks 15% of tokens\n",
    "3. Visualize which tokens are masked\n",
    "4. Discuss why we don't mask 100% or 5% of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1 - Your code here\n",
    "\n",
    "# Step 1: Create your dataset\n",
    "sentences = [\n",
    "    # Add 5 sentences about African wildlife\n",
    "]\n",
    "\n",
    "# Step 2: Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 3: Write a function to mask tokens\n",
    "def mask_tokens(text, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Mask random tokens in the text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        mask_prob: Probability of masking each token\n",
    "    \n",
    "    Returns:\n",
    "        masked_text: Text with [MASK] tokens\n",
    "        original_text: Original text\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 4: Test your function\n",
    "for sent in sentences:\n",
    "    masked, original = mask_tokens(sent)\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Masked:   {masked}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exercise 1\n",
    "\n",
    "Answer these questions in the markdown cell below:\n",
    "\n",
    "1. What happens if you mask 100% of tokens? Why would this be problematic?\n",
    "2. What happens if you mask only 5% of tokens?\n",
    "3. Why is 15% a good balance?\n",
    "4. In BERT, why do we sometimes replace masked tokens with random tokens instead of [MASK]?\n",
    "\n",
    "**Your Answers:**\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2: Encoder vs Decoder - Practical Comparison\n",
    "\n",
    "## Objective\n",
    "Understand when to use encoders vs decoders through practical examples.\n",
    "\n",
    "## Task\n",
    "For each task below, decide whether an encoder or decoder is more appropriate and explain why:\n",
    "\n",
    "1. Classifying news articles into categories\n",
    "2. Generating creative stories\n",
    "3. Extracting named entities from text\n",
    "4. Completing code snippets\n",
    "5. Determining if two sentences are paraphrases\n",
    "6. Writing product descriptions\n",
    "7. Question answering (extractive)\n",
    "8. Chatbot responses\n",
    "9. Text summarization\n",
    "10. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers for Exercise 2\n",
    "\n",
    "Fill in the table below:\n",
    "\n",
    "| Task | Model Type (Encoder/Decoder) | Reason |\n",
    "|------|------------------------------|--------|\n",
    "| 1. News classification | | |\n",
    "| 2. Story generation | | |\n",
    "| 3. Named entity recognition | | |\n",
    "| 4. Code completion | | |\n",
    "| 5. Paraphrase detection | | |\n",
    "| 6. Product descriptions | | |\n",
    "| 7. Question answering | | |\n",
    "| 8. Chatbot | | |\n",
    "| 9. Summarization | | |\n",
    "| 10. Sentiment analysis | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 3: Fine-tuning for Named Entity Recognition (NER)\n",
    "\n",
    "## Objective\n",
    "Fine-tune a model for token classification (NER) on MasakhaNER dataset.\n",
    "\n",
    "## Background\n",
    "MasakhaNER is a dataset for named entity recognition in African languages. It identifies:\n",
    "- PER (Person names)\n",
    "- ORG (Organizations)\n",
    "- LOC (Locations)\n",
    "- DATE (Dates)\n",
    "\n",
    "## Task\n",
    "1. Load MasakhaNER for Hausa or Yoruba\n",
    "2. Fine-tune AfroXLMR for NER\n",
    "3. Evaluate on test set\n",
    "4. Test on custom examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3 - Your code here\n",
    "\n",
    "# Choose your language\n",
    "NER_LANGUAGE = \"hau\"  # Options: hau (Hausa), yor (Yoruba), swa (Swahili), etc.\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "print(f\"Loading MasakhaNER dataset for {NER_LANGUAGE}...\")\n",
    "# HINT: Use load_dataset(\"masakhaner\", NER_LANGUAGE)\n",
    "\n",
    "# Step 2: Explore the dataset\n",
    "# - Print dataset structure\n",
    "# - Show example with tokens and NER tags\n",
    "# - Count label distribution\n",
    "\n",
    "# Step 3: Load model and tokenizer\n",
    "# HINT: Use AutoModelForTokenClassification\n",
    "# HINT: You need to specify num_labels based on the dataset\n",
    "\n",
    "# Step 4: Tokenize the dataset\n",
    "# HINT: For NER, you need to align labels with tokens\n",
    "\n",
    "# Step 5: Set up training\n",
    "# - Define training arguments\n",
    "# - Create data collator (DataCollatorForTokenClassification)\n",
    "# - Define compute_metrics function\n",
    "\n",
    "# Step 6: Train the model\n",
    "\n",
    "# Step 7: Evaluate on test set\n",
    "\n",
    "# Step 8: Test on custom examples\n",
    "test_sentences = [\n",
    "    # Add test sentences in your chosen language\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exercise 3\n",
    "\n",
    "1. How does NER differ from sentiment analysis in terms of the prediction task?\n",
    "2. Why is token alignment important in NER?\n",
    "3. Which entity type was hardest for your model to identify? Why?\n",
    "4. How could you improve NER performance?\n",
    "\n",
    "**Your Answers:**\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 4: Comparing Pre-training Objectives\n",
    "\n",
    "## Objective\n",
    "Experiment with different masking probabilities in MLM.\n",
    "\n",
    "## Task\n",
    "1. Pre-train three small BERT models with different mask probabilities:\n",
    "   - Model A: 5% masking\n",
    "   - Model B: 15% masking (standard)\n",
    "   - Model C: 30% masking\n",
    "2. Use the same toy dataset and training steps\n",
    "3. Compare their performance on a fill-mask task\n",
    "4. Plot training loss curves\n",
    "5. Analyze which performs best and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 4 - Your code here\n",
    "\n",
    "# Step 1: Create a toy corpus (can reuse from Lab 1 or create new one)\n",
    "toy_corpus = [\n",
    "    # Add sentences\n",
    "]\n",
    "\n",
    "# Step 2: Train tokenizer\n",
    "\n",
    "# Step 3: Function to train model with specific mask probability\n",
    "def train_model_with_mask_prob(corpus, mask_prob, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Train a BERT model with specified masking probability.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of text strings\n",
    "        mask_prob: Masking probability (0.0 to 1.0)\n",
    "        num_epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        training_loss: List of training losses\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 4: Train three models\n",
    "print(\"Training Model A (5% masking)...\")\n",
    "# model_a, loss_a = train_model_with_mask_prob(toy_corpus, 0.05)\n",
    "\n",
    "print(\"Training Model B (15% masking)...\")\n",
    "# model_b, loss_b = train_model_with_mask_prob(toy_corpus, 0.15)\n",
    "\n",
    "print(\"Training Model C (30% masking)...\")\n",
    "# model_c, loss_c = train_model_with_mask_prob(toy_corpus, 0.30)\n",
    "\n",
    "# Step 5: Compare performance\n",
    "# - Test on fill-mask tasks\n",
    "# - Plot loss curves\n",
    "# - Analyze results\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Exercise 4\n",
    "\n",
    "Fill in your observations:\n",
    "\n",
    "1. **Training Loss Comparison:**\n",
    "   - Which model had the lowest final training loss?\n",
    "   - How did masking probability affect convergence speed?\n",
    "\n",
    "2. **Fill-mask Performance:**\n",
    "   - Which model made the best predictions?\n",
    "   - Did higher masking probability help or hurt?\n",
    "\n",
    "3. **Why 15% is Standard:**\n",
    "   - Based on your results, why might 15% be the sweet spot?\n",
    "\n",
    "**Your Analysis:**\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 5: Multi-language Fine-tuning Comparison\n",
    "\n",
    "## Objective\n",
    "Compare AfroXLMR performance across different African languages.\n",
    "\n",
    "## Task\n",
    "1. Fine-tune AfroXLMR on AfriSenti for THREE different languages\n",
    "2. Keep all hyperparameters identical\n",
    "3. Compare test set performance\n",
    "4. Analyze why some languages perform better than others\n",
    "5. Create visualization comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 5 - Your code here\n",
    "\n",
    "# Languages to compare\n",
    "LANGUAGES = [\"ha\", \"ig\", \"yo\"]  # Hausa, Igbo, Yoruba\n",
    "\n",
    "# Step 1: Function to fine-tune and evaluate for a language\n",
    "def finetune_and_evaluate(language):\n",
    "    \"\"\"\n",
    "    Fine-tune AfroXLMR for sentiment analysis in a specific language.\n",
    "    \n",
    "    Args:\n",
    "        language: Language code\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with test metrics\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 2: Fine-tune for each language\n",
    "results = {}\n",
    "for lang in LANGUAGES:\n",
    "    print(f\"\\nFine-tuning for {lang}...\")\n",
    "    results[lang] = finetune_and_evaluate(lang)\n",
    "\n",
    "# Step 3: Create comparison visualization\n",
    "# - Bar chart comparing accuracy, F1, precision, recall\n",
    "# - Table showing all metrics\n",
    "\n",
    "# Step 4: Analysis\n",
    "# - Which language performed best?\n",
    "# - Why might there be differences?\n",
    "# - What factors affect cross-lingual performance?\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Exercise 5\n",
    "\n",
    "1. **Performance Ranking:**\n",
    "   - Rank the languages by F1-score\n",
    "   - What was the performance difference between best and worst?\n",
    "\n",
    "2. **Possible Explanations:**\n",
    "   - Dataset size differences?\n",
    "   - Language similarity to pre-training data?\n",
    "   - Inherent language complexity?\n",
    "   - Data quality?\n",
    "\n",
    "3. **Improvement Strategies:**\n",
    "   - How would you improve performance for the lowest-scoring language?\n",
    "\n",
    "**Your Analysis:**\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 6: Hyperparameter Tuning\n",
    "\n",
    "## Objective\n",
    "Experiment with different hyperparameters and understand their effects.\n",
    "\n",
    "## Task\n",
    "1. Choose one language from AfriSenti\n",
    "2. Try THREE different learning rates: 1e-5, 2e-5, 5e-5\n",
    "3. Try THREE different batch sizes: 8, 16, 32\n",
    "4. Keep epochs constant at 3\n",
    "5. Document which combination works best\n",
    "6. Explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 6 - Your code here\n",
    "\n",
    "# Hyperparameters to test\n",
    "LEARNING_RATES = [1e-5, 2e-5, 5e-5]\n",
    "BATCH_SIZES = [8, 16, 32]\n",
    "LANGUAGE = \"ha\"  # Choose your language\n",
    "\n",
    "# Step 1: Load dataset once\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 2: Grid search function\n",
    "def run_experiment(lr, batch_size):\n",
    "    \"\"\"\n",
    "    Train model with specific hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with test metrics\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 3: Run experiments\n",
    "experiment_results = []\n",
    "for lr in LEARNING_RATES:\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\nTesting LR={lr}, Batch Size={batch_size}\")\n",
    "        result = run_experiment(lr, batch_size)\n",
    "        experiment_results.append({\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            **result\n",
    "        })\n",
    "\n",
    "# Step 4: Visualize results\n",
    "# - Heatmap showing F1-scores for different combinations\n",
    "# - Table with all results\n",
    "\n",
    "# Step 5: Identify best configuration\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Exercise 6\n",
    "\n",
    "1. **Best Configuration:**\n",
    "   - What learning rate worked best?\n",
    "   - What batch size worked best?\n",
    "   - What was the final F1-score?\n",
    "\n",
    "2. **Observations:**\n",
    "   - How did increasing learning rate affect results?\n",
    "   - How did increasing batch size affect results?\n",
    "   - Were there any surprising findings?\n",
    "\n",
    "3. **Trade-offs:**\n",
    "   - Larger batch size: faster training but...\n",
    "   - Higher learning rate: faster convergence but...\n",
    "\n",
    "**Your Analysis:**\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 7: Error Analysis and Improvement\n",
    "\n",
    "## Objective\n",
    "Analyze model errors and develop strategies for improvement.\n",
    "\n",
    "## Task\n",
    "1. Fine-tune AfroXLMR on one language\n",
    "2. Find 10 examples the model got wrong\n",
    "3. Categorize the types of errors\n",
    "4. Propose strategies to fix each error type\n",
    "5. Implement one improvement strategy and measure impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 7 - Your code here\n",
    "\n",
    "# Step 1: Train model and get predictions\n",
    "LANGUAGE = \"ha\"  # Choose language\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 2: Find misclassified examples\n",
    "def find_errors(predictions, true_labels, texts, n=10):\n",
    "    \"\"\"\n",
    "    Find misclassified examples.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        true_labels: Ground truth labels\n",
    "        texts: Original texts\n",
    "        n: Number of errors to find\n",
    "    \n",
    "    Returns:\n",
    "        errors: List of error examples\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 3: Categorize errors\n",
    "# Examples of error categories:\n",
    "# - Sarcasm/irony not detected\n",
    "# - Ambiguous sentiment\n",
    "# - Wrong neutral vs negative\n",
    "# - Wrong neutral vs positive\n",
    "# - Language-specific expressions\n",
    "\n",
    "# Step 4: Analyze each error\n",
    "print(\"Error Analysis:\")\n",
    "print(\"=\"*80)\n",
    "# For each error:\n",
    "# - Print the text\n",
    "# - Show true label vs predicted label\n",
    "# - Categorize the error\n",
    "# - Explain why the model might have failed\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 5: Improvement strategy\n",
    "# Choose one strategy:\n",
    "# - Data augmentation\n",
    "# - Class weights for imbalanced data\n",
    "# - Longer training\n",
    "# - Different model architecture\n",
    "\n",
    "# Implement and measure impact\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis Report\n",
    "\n",
    "Fill in your findings:\n",
    "\n",
    "**Error Categories Found:**\n",
    "1. Category 1: [Description] - [Count] examples\n",
    "2. Category 2: [Description] - [Count] examples\n",
    "3. Category 3: [Description] - [Count] examples\n",
    "\n",
    "**Most Common Error Type:**\n",
    "- Description:\n",
    "- Why it happens:\n",
    "- How to fix:\n",
    "\n",
    "**Improvement Strategy Tested:**\n",
    "- Strategy:\n",
    "- Implementation:\n",
    "- Results:\n",
    "- Before F1:\n",
    "- After F1:\n",
    "- Improvement:\n",
    "\n",
    "**Your Report:**\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Challenge Exercise 8: Build a Sentiment Analysis Pipeline\n",
    "\n",
    "## Objective\n",
    "Create a production-ready sentiment analysis pipeline.\n",
    "\n",
    "## Task\n",
    "Build a complete pipeline that:\n",
    "1. Accepts text input in any of 3 African languages\n",
    "2. Detects the language automatically\n",
    "3. Uses the appropriate fine-tuned model\n",
    "4. Returns sentiment with confidence score\n",
    "5. Handles edge cases (empty text, very long text, mixed languages)\n",
    "6. Provides explanations for predictions\n",
    "\n",
    "## Bonus\n",
    "- Add batch processing capability\n",
    "- Create a simple web interface (optional)\n",
    "- Save predictions to a file\n",
    "- Add visualization of confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Challenge Exercise 8 - Your code here\n",
    "\n",
    "class MultilingualSentimentPipeline:\n",
    "    \"\"\"Production-ready multilingual sentiment analysis pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, languages, model_paths):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            languages: List of supported language codes\n",
    "            model_paths: Dictionary mapping language codes to model paths\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        \"\"\"\n",
    "        Detect the language of input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "        \n",
    "        Returns:\n",
    "            language: Detected language code\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def predict(self, text, language=None):\n",
    "        \"\"\"\n",
    "        Predict sentiment for input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            language: Language code (optional, will auto-detect if None)\n",
    "        \n",
    "        Returns:\n",
    "            result: Dictionary with prediction, confidence, and explanation\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def batch_predict(self, texts, languages=None):\n",
    "        \"\"\"\n",
    "        Predict sentiments for multiple texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            languages: List of language codes (optional)\n",
    "        \n",
    "        Returns:\n",
    "            results: List of prediction dictionaries\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def explain_prediction(self, text, prediction):\n",
    "        \"\"\"\n",
    "        Provide explanation for prediction.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            prediction: Prediction dictionary\n",
    "        \n",
    "        Returns:\n",
    "            explanation: String explaining the prediction\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "pipeline = MultilingualSentimentPipeline(\n",
    "    languages=[\"ha\", \"ig\", \"yo\"],\n",
    "    model_paths={\n",
    "        \"ha\": \"/path/to/hausa/model\",\n",
    "        \"ig\": \"/path/to/igbo/model\",\n",
    "        \"yo\": \"/path/to/yoruba/model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the pipeline\n",
    "test_texts = [\n",
    "    # Add test texts in different languages\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = pipeline.predict(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Language: {result['language']}\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.2%})\")\n",
    "    print(f\"Explanation: {result['explanation']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Reflection\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "Through these exercises, you should now understand:\n",
    "\n",
    "1. **Pre-training Strategies**\n",
    "   - How MLM works and why masking probability matters\n",
    "   - Differences between encoder and decoder pre-training\n",
    "   - When to use each architecture\n",
    "\n",
    "2. **Fine-tuning Process**\n",
    "   - How to adapt pre-trained models to specific tasks\n",
    "   - Importance of hyperparameter tuning\n",
    "   - Cross-lingual transfer learning\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Multiple metrics for comprehensive evaluation\n",
    "   - Error analysis techniques\n",
    "   - Strategies for improvement\n",
    "\n",
    "4. **Practical Skills**\n",
    "   - Working with Hugging Face transformers\n",
    "   - Training and evaluation workflows\n",
    "   - Building production pipelines\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "Answer these questions to solidify your learning:\n",
    "\n",
    "1. **Conceptual Understanding:**\n",
    "   - In your own words, explain transfer learning and why it's important\n",
    "   - What's the key difference between pre-training and fine-tuning?\n",
    "\n",
    "2. **Practical Application:**\n",
    "   - For your final project, would you use an encoder or decoder model? Why?\n",
    "   - What hyperparameters would you tune first? Why?\n",
    "\n",
    "3. **Real-world Scenarios:**\n",
    "   - How would you deploy a sentiment analysis model for production use?\n",
    "   - What challenges might you face with low-resource African languages?\n",
    "\n",
    "**Your Reflections:**\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Additional Resources\n",
    "\n",
    "## Datasets for Practice\n",
    "- **AfriSenti**: Sentiment analysis in 14 African languages\n",
    "- **MasakhaNER**: Named entity recognition in 21 African languages\n",
    "- **MasakhaPOS**: Part-of-speech tagging in 20 African languages\n",
    "- **MAFAND**: News article classification\n",
    "\n",
    "## Pre-trained Models\n",
    "- **AfroXLMR**: Multilingual encoder for African languages\n",
    "- **AfriBERTa**: BERT-based model for African languages\n",
    "- **mBERT**: Multilingual BERT (includes some African languages)\n",
    "- **XLM-RoBERTa**: Cross-lingual model covering 100+ languages\n",
    "\n",
    "## Further Reading\n",
    "- Hugging Face Transformers documentation\n",
    "- BERT paper: \"Pre-training of Deep Bidirectional Transformers\"\n",
    "- GPT paper: \"Improving Language Understanding by Generative Pre-Training\"\n",
    "- \"How NLP Cracked Transfer Learning\" blog post\n",
    "\n",
    "## Community\n",
    "- Masakhane NLP community: https://www.masakhane.io/\n",
    "- Hugging Face forums: https://discuss.huggingface.co/\n",
    "- AfricaNLP workshop at major conferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Next Steps\n",
    "\n",
    "Ready to go further? Try these advanced challenges:\n",
    "\n",
    "1. **Multi-task Learning**\n",
    "   - Train one model for both sentiment analysis and NER\n",
    "   - Compare to task-specific models\n",
    "\n",
    "2. **Few-shot Learning**\n",
    "   - Fine-tune with only 10 examples per class\n",
    "   - Use data augmentation techniques\n",
    "\n",
    "3. **Model Compression**\n",
    "   - Use knowledge distillation to create smaller models\n",
    "   - Compare size vs performance trade-offs\n",
    "\n",
    "4. **Zero-shot Cross-lingual Transfer**\n",
    "   - Train on one language, test on another\n",
    "   - Analyze which language pairs transfer best\n",
    "\n",
    "5. **Custom Pre-training**\n",
    "   - Collect domain-specific data (e.g., medical texts)\n",
    "   - Continue pre-training AfroXLMR on this data\n",
    "   - Fine-tune for domain-specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You've completed the practice exercises for pre-training strategies and fine-tuning. You now have hands-on experience with:\n",
    "\n",
    "- ✅ Masked Language Modeling (MLM)\n",
    "- ✅ Causal Language Modeling (CLM)\n",
    "- ✅ Fine-tuning for multiple tasks\n",
    "- ✅ Hyperparameter optimization\n",
    "- ✅ Error analysis and improvement\n",
    "- ✅ Multi-lingual NLP for African languages\n",
    "\n",
    "Keep practicing and exploring! The field of NLP for African languages is growing rapidly, and your skills can make a real impact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
