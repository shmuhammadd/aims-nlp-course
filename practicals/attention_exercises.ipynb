{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Self-Attention: Exercises\n",
    "## NLP and LLMs Course - AIMS South Africa 2025\n",
    "\n",
    "### Instructions\n",
    "Complete all TODO sections in this notebook. Test your implementations with the provided test cases. \n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Implement dot-product attention mechanism\n",
    "2. Implement scaled dot-product attention\n",
    "3. Implement self-attention layer\n",
    "4. Build an attention-based encoder\n",
    "5. Understand multi-head attention\n",
    "\n",
    "**Submission:** Complete notebook with all cells executed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration - GPU support\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    print(\"‚úì GPU acceleration enabled!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU (slower but works fine)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Dot-Product Attention (20 points)\n",
    "\n",
    "Implement the basic dot-product attention mechanism.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "scores = Q @ K^T\n",
    "attention_weights = softmax(scores)\n",
    "output = attention_weights @ V\n",
    "```\n",
    "\n",
    "**Args:**\n",
    "- `query`: (batch_size, hidden_dim)\n",
    "- `keys`: (batch_size, seq_len, hidden_dim)\n",
    "- `values`: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "**Returns:**\n",
    "- `attention_output`: (batch_size, hidden_dim)\n",
    "- `attention_weights`: (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    Implement dot-product attention mechanism.\n",
    "    \"\"\"\n",
    "    # TODO: Step 1 - Compute attention scores (query @ keys^T)\n",
    "    # Hint: query needs to be (batch_size, 1, hidden_dim) for batch matrix multiplication\n",
    "    # Hint: keys need to be transposed to (batch_size, hidden_dim, seq_len)\n",
    "    scores = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Step 2 - Apply softmax to get attention weights\n",
    "    # The weights should sum to 1 along the sequence dimension\n",
    "    attention_weights = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Step 3 - Compute weighted sum of values\n",
    "    # Hint: attention_weights should be (batch_size, 1, seq_len)\n",
    "    attention_output = None  # YOUR CODE HERE\n",
    "    \n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "print(\"Testing dot_product_attention...\")\n",
    "batch_size, seq_len, hidden_dim = 2, 4, 8\n",
    "query = torch.randn(batch_size, hidden_dim)\n",
    "keys = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "values = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "output, weights = dot_product_attention(query, keys, values)\n",
    "\n",
    "# Assertions\n",
    "assert output.shape == (batch_size, hidden_dim), f\"Output shape should be {(batch_size, hidden_dim)}, got {output.shape}\"\n",
    "assert weights.shape == (batch_size, seq_len), f\"Weights shape should be {(batch_size, seq_len)}, got {weights.shape}\"\n",
    "assert torch.allclose(weights.sum(dim=1), torch.ones(batch_size), atol=1e-6), \"Attention weights should sum to 1\"\n",
    "\n",
    "print(\"‚úì All tests passed!\")\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"Attention weights (first sample): {weights[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Scaled Dot-Product Attention (20 points)\n",
    "\n",
    "Implement scaled dot-product attention used in Transformers.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "scores = (Q @ K^T) / sqrt(d_k)\n",
    "attention_weights = softmax(scores)\n",
    "output = attention_weights @ V\n",
    "```\n",
    "\n",
    "The scaling factor prevents the dot products from growing too large in high dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, keys, values, mask=None):\n",
    "    \"\"\"\n",
    "    Implement scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch_size, hidden_dim)\n",
    "        keys: (batch_size, seq_len, hidden_dim)\n",
    "        values: (batch_size, seq_len, hidden_dim)\n",
    "        mask: Optional (batch_size, seq_len) mask (1 for valid, 0 for masked)\n",
    "    \n",
    "    Returns:\n",
    "        attention_output: (batch_size, hidden_dim)\n",
    "        attention_weights: (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    # TODO: Get the dimension for scaling\n",
    "    d_k = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compute attention scores and scale by sqrt(d_k)\n",
    "    scores = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        pass  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Apply softmax to get attention weights\n",
    "    attention_weights = None  # YOUR CODE HERE\n",
    "    \n",
    "    # TODO: Compute weighted sum of values\n",
    "    attention_output = None  # YOUR CODE HERE\n",
    "    \n",
    "    return attention_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "print(\"Testing scaled_dot_product_attention...\")\n",
    "output_scaled, weights_scaled = scaled_dot_product_attention(query, keys, values)\n",
    "\n",
    "# Test with mask\n",
    "mask = torch.ones(batch_size, seq_len)\n",
    "mask[:, 2:] = 0  # Mask out positions after index 2\n",
    "output_masked, weights_masked = scaled_dot_product_attention(query, keys, values, mask)\n",
    "\n",
    "# Assertions\n",
    "assert output_scaled.shape == (batch_size, hidden_dim), f\"Output shape incorrect\"\n",
    "assert weights_scaled.shape == (batch_size, seq_len), f\"Weights shape incorrect\"\n",
    "assert torch.allclose(weights_scaled.sum(dim=1), torch.ones(batch_size), atol=1e-6), \"Weights should sum to 1\"\n",
    "assert torch.allclose(weights_masked[:, 2:], torch.zeros(batch_size, seq_len-2), atol=1e-6), \"Masked positions should have 0 weight\"\n",
    "\n",
    "print(\"‚úì All tests passed!\")\n",
    "print(f\"Weights without mask: {weights_scaled[0]}\")\n",
    "print(f\"Weights with mask: {weights_masked[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Self-Attention Layer (30 points)\n",
    "\n",
    "Implement a complete self-attention layer with learnable projections for Q, K, and V.\n",
    "\n",
    "**Key Points:**\n",
    "- All Q, K, V come from the same input sequence\n",
    "- Use learnable linear projections: W_q, W_k, W_v\n",
    "- Output should have the same shape as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Self-Attention layer.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Embedding dimension\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # TODO: Initialize learnable projection matrices\n",
    "        # Hint: Use nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_q = None  # YOUR CODE HERE\n",
    "        self.W_k = None  # YOUR CODE HERE\n",
    "        self.W_v = None  # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, embed_dim)\n",
    "            mask: Optional mask (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, embed_dim)\n",
    "            attention_weights: (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # TODO: Project input to queries, keys, and values\n",
    "        Q = None  # YOUR CODE HERE - shape: (batch_size, seq_len, embed_dim)\n",
    "        K = None  # YOUR CODE HERE - shape: (batch_size, seq_len, embed_dim)\n",
    "        V = None  # YOUR CODE HERE - shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # TODO: Compute attention scores: Q @ K^T\n",
    "        # Hint: Use torch.bmm or torch.matmul\n",
    "        scores = None  # YOUR CODE HERE - shape: (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # TODO: Scale scores\n",
    "        scores = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply mask if provided\n",
    "        if mask is not None:\n",
    "            pass  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply softmax along the last dimension\n",
    "        attention_weights = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Compute weighted sum: attention_weights @ V\n",
    "        output = None  # YOUR CODE HERE - shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "print(\"Testing SelfAttention...\")\n",
    "batch_size, seq_len, embed_dim = 2, 5, 8\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "self_attn = SelfAttention(embed_dim)\n",
    "output, attn_weights = self_attn(x)\n",
    "\n",
    "# Assertions\n",
    "assert output.shape == (batch_size, seq_len, embed_dim), f\"Output shape should be {(batch_size, seq_len, embed_dim)}, got {output.shape}\"\n",
    "assert attn_weights.shape == (batch_size, seq_len, seq_len), f\"Attention weights shape should be {(batch_size, seq_len, seq_len)}, got {attn_weights.shape}\"\n",
    "assert torch.allclose(attn_weights.sum(dim=-1), torch.ones(batch_size, seq_len), atol=1e-6), \"Attention weights should sum to 1 along last dimension\"\n",
    "\n",
    "print(\"‚úì All tests passed!\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Multi-Head Attention (30 points)\n",
    "\n",
    "Implement multi-head attention, which allows the model to attend to different representation subspaces.\n",
    "\n",
    "**Key Idea:**\n",
    "- Split the embedding dimension into multiple heads\n",
    "- Apply attention in parallel for each head\n",
    "- Concatenate the outputs\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "head_i = Attention(Q_i, K_i, V_i)\n",
    "MultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W_O\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Embedding dimension (must be divisible by num_heads)\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # TODO: Initialize projection matrices\n",
    "        self.W_q = None  # YOUR CODE HERE\n",
    "        self.W_k = None  # YOUR CODE HERE\n",
    "        self.W_v = None  # YOUR CODE HERE\n",
    "        self.W_o = None  # YOUR CODE HERE - output projection\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, seq_len, embed_dim)\n",
    "            mask: Optional mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, embed_dim)\n",
    "            attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # TODO: Project to Q, K, V\n",
    "        Q = None  # YOUR CODE HERE\n",
    "        K = None  # YOUR CODE HERE\n",
    "        V = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Reshape for multi-head attention\n",
    "        # Split embed_dim into (num_heads, head_dim)\n",
    "        # Final shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = None  # YOUR CODE HERE\n",
    "        K = None  # YOUR CODE HERE\n",
    "        V = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Compute attention scores for all heads\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        scores = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Scale\n",
    "        scores = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply mask if provided\n",
    "        if mask is not None:\n",
    "            pass  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply softmax\n",
    "        attention_weights = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply attention to values\n",
    "        # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        attended = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Reshape back to (batch_size, seq_len, embed_dim)\n",
    "        # Hint: transpose and reshape to concatenate heads\n",
    "        attended = None  # YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Apply output projection\n",
    "        output = None  # YOUR CODE HERE\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "print(\"Testing MultiHeadAttention...\")\n",
    "batch_size, seq_len, embed_dim, num_heads = 2, 5, 16, 4\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "# Assertions\n",
    "assert output.shape == (batch_size, seq_len, embed_dim), f\"Output shape incorrect\"\n",
    "assert attn_weights.shape == (batch_size, num_heads, seq_len, seq_len), f\"Attention weights shape incorrect\"\n",
    "assert torch.allclose(attn_weights.sum(dim=-1), torch.ones(batch_size, num_heads, seq_len), atol=1e-6), \"Weights should sum to 1\"\n",
    "\n",
    "print(\"‚úì All tests passed!\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Causal Masking (10 bonus points)\n",
    "\n",
    "Implement a causal mask for autoregressive generation (like in GPT).\n",
    "\n",
    "**Purpose:** Prevent the model from attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) mask.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) boolean tensor\n",
    "              True for valid positions, False for masked\n",
    "    \"\"\"\n",
    "    # TODO: Create lower triangular mask\n",
    "    # Hint: Use torch.tril\n",
    "    mask = None  # YOUR CODE HERE\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test causal mask\n",
    "mask = create_causal_mask(5)\n",
    "print(\"Causal mask:\")\n",
    "print(mask.int())\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(mask.int().numpy(), cmap='Blues', cbar=False, square=True, \n",
    "            xticklabels=range(1, 6), yticklabels=range(1, 6))\n",
    "plt.title('Causal Attention Mask\\n(1 = can attend, 0 = masked)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before submitting, ensure:\n",
    "\n",
    "- [ ] All TODO sections are completed\n",
    "- [ ] All test cells run without errors\n",
    "- [ ] All assertions pass\n",
    "- [ ] Code is well-commented\n",
    "- [ ] You understand what each part does\n",
    "\n",
    "**Total Points: 100 + 10 bonus**\n",
    "\n",
    "Good luck! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}