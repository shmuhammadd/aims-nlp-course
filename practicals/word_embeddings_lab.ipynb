{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Training: Word2Vec, FastText and GloVe\n",
    "## NLP at AIMS South Africa 2025\n",
    "### Based on Lecture 04: Word Representation\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Word2Vec (CBOW and Skip-gram) training\n",
    "2. FastText training\n",
    "3. GloVe training\n",
    "4. Both dummy data and real datasets from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install gensim datasets numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from datasets import load_dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Getting Training Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Dummy dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple corpus\n",
    "dummy_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are animals\",\n",
    "    \"the cat and the dog are friends\",\n",
    "    \"a cat is a pet\",\n",
    "    \"a dog is a pet\",\n",
    "    \"cats like to sit on mats\",\n",
    "    \"dogs like to play with balls\",\n",
    "    \"the animal sat on the ground\",\n",
    "    \"pets are good friends\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "dummy_sentences = [sentence.split() for sentence in dummy_corpus]\n",
    "print(f\"Number of sentences: {len(dummy_sentences)}\")\n",
    "print(f\"Example sentence: {dummy_sentences[0]}\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = set(word for sentence in dummy_sentences for word in sentence)\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(f\"Vocabulary: {sorted(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Real dataset (HuggingFace)\n",
    "---\n",
    "\n",
    "We'll use the AG News dataset - a collection of news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a moderately sized dataset - using AG News (news classification)\n",
    "print(\"Loading AG News dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:5000]\")  # Using 5000 samples\n",
    "\n",
    "# Prepare sentences\n",
    "real_sentences = []\n",
    "for example in dataset:\n",
    "    # Tokenize simply by splitting on whitespace and converting to lowercase\n",
    "    words = example['text'].lower().split()\n",
    "    if len(words) > 3:  # Filter very short sentences\n",
    "        real_sentences.append(words)\n",
    "\n",
    "print(f\"Number of sentences: {len(real_sentences)}\")\n",
    "print(f\"Example sentence: {' '.join(real_sentences[0][:15])}...\")\n",
    "\n",
    "# Build vocabulary stats\n",
    "all_words = [word for sentence in real_sentences for word in sentence]\n",
    "vocab_real = set(all_words)\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocab_real)}\")\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "print(f\"\\nMost common words:\")\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 0: Baseline - TF-IDF Representation\n",
    "---\n",
    "\n",
    "Before diving into word embeddings, let's understand traditional TF-IDF representation.\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "- **TF**: How often a word appears in a document\n",
    "- **IDF**: How rare/common a word is across all documents\n",
    "- **Result**: Sparse vectors that weight important words\n",
    "\n",
    "**Limitations:**\n",
    "- High dimensionality (vocabulary size)\n",
    "- Sparse representations (mostly zeros)\n",
    "- No semantic meaning (\"cat\" and \"kitten\" are completely different)\n",
    "- Cannot handle out-of-vocabulary words\n",
    "\n",
    "**Why Word Embeddings?**\n",
    "Word embeddings solve these problems by learning dense, semantic representations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1: TF-IDF on Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create TF-IDF vectors from dummy corpus\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=50,  # Limit features for visualization\n",
    "    lowercase=True,\n",
    "    stop_words=None   # Keep all words for comparison\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dummy_corpus)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF Vectorizer trained!\")\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"\\nVocabulary: {list(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TF-IDF weights for a specific document\n",
    "doc_idx = 0  # \"the cat sat on the mat\"\n",
    "doc_vector = tfidf_matrix[doc_idx].toarray()[0]\n",
    "\n",
    "print(f\"Document: '{dummy_corpus[doc_idx]}'\\n\")\n",
    "print(\"TF-IDF weights:\")\n",
    "word_weights = [(feature_names[i], doc_vector[i]) for i in range(len(feature_names)) if doc_vector[i] > 0]\n",
    "word_weights.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, weight in word_weights:\n",
    "    print(f\"  {word}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar documents using TF-IDF\n",
    "query_doc = \"the cat sat on the mat\"\n",
    "query_vector = tfidf_vectorizer.transform([query_doc])\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
    "\n",
    "print(f\"Query: '{query_doc}'\\n\")\n",
    "print(\"Most similar documents:\")\n",
    "for idx in np.argsort(similarities)[::-1][:5]:\n",
    "    print(f\"  [{idx}] {dummy_corpus[idx]}: {similarities[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Limitations Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate lack of semantic understanding\n",
    "print(\"TF-IDF LIMITATION: No Semantic Understanding\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create word vectors by averaging document vectors where word appears\n",
    "def get_word_representation_tfidf(word, vectorizer, matrix, corpus):\n",
    "    \"\"\"Get a pseudo-vector for a word by averaging TF-IDF vectors of documents containing it\"\"\"\n",
    "    word_docs = [i for i, doc in enumerate(corpus) if word in doc]\n",
    "    if not word_docs:\n",
    "        return None\n",
    "    return matrix[word_docs].mean(axis=0).A1\n",
    "\n",
    "# Get representations\n",
    "cat_tfidf = get_word_representation_tfidf('cat', tfidf_vectorizer, tfidf_matrix, dummy_corpus)\n",
    "dog_tfidf = get_word_representation_tfidf('dog', tfidf_vectorizer, tfidf_matrix, dummy_corpus)\n",
    "mat_tfidf = get_word_representation_tfidf('mat', tfidf_vectorizer, tfidf_matrix, dummy_corpus)\n",
    "\n",
    "if cat_tfidf is not None and dog_tfidf is not None and mat_tfidf is not None:\n",
    "    # Compute similarities\n",
    "    cat_dog_sim = cosine_similarity([cat_tfidf], [dog_tfidf])[0][0]\n",
    "    cat_mat_sim = cosine_similarity([cat_tfidf], [mat_tfidf])[0][0]\n",
    "    \n",
    "    print(f\"Similarity (cat, dog): {cat_dog_sim:.4f}\")\n",
    "    print(f\"Similarity (cat, mat): {cat_mat_sim:.4f}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Problem: TF-IDF doesn't capture that 'cat' and 'dog' are semantically related!\")\n",
    "    print(\"    They appear in similar contexts but TF-IDF only looks at word co-occurrence.\")\n",
    "else:\n",
    "    print(\"Some words not found in corpus\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ Word embeddings solve this by learning semantic relationships!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TF-IDF sparsity\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot sparse matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.spy(tfidf_matrix, markersize=5, aspect='auto')\n",
    "plt.title('TF-IDF Matrix Sparsity\\n(Blue = Non-zero values)')\n",
    "plt.xlabel('Features (Words)')\n",
    "plt.ylabel('Documents')\n",
    "\n",
    "# Plot feature distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "feature_counts = np.array((tfidf_matrix > 0).sum(axis=0)).flatten()\n",
    "plt.bar(range(len(feature_counts)), feature_counts)\n",
    "plt.title('Word Frequency Across Documents')\n",
    "plt.xlabel('Word Index')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(\"Most TF-IDF values are zero (sparse representation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2: TF-IDF on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for TF-IDF\n",
    "real_corpus_text = [' '.join(sentence) for sentence in real_sentences]\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "real_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit to top 1000 features\n",
    "    max_df=0.8,         # Ignore words that appear in >80% of documents\n",
    "    min_df=5,           # Ignore words that appear in <5 documents\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "real_tfidf_matrix = real_tfidf_vectorizer.fit_transform(real_corpus_text)\n",
    "real_feature_names = real_tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF on Real Data:\")\n",
    "print(f\"Vocabulary size: {len(real_feature_names)}\")\n",
    "print(f\"Matrix shape: {real_tfidf_matrix.shape}\")\n",
    "print(f\"Matrix sparsity: {(1 - real_tfidf_matrix.nnz / (real_tfidf_matrix.shape[0] * real_tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"\\nTop 20 features by average TF-IDF:\")\n",
    "\n",
    "# Calculate average TF-IDF per feature\n",
    "mean_tfidf = np.array(real_tfidf_matrix.mean(axis=0)).flatten()\n",
    "top_indices = np.argsort(mean_tfidf)[::-1][:20]\n",
    "\n",
    "for idx in top_indices:\n",
    "    print(f\"  {real_feature_names[idx]}: {mean_tfidf[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document similarity search with TF-IDF\n",
    "query = \"business market economy\"\n",
    "query_vector = real_tfidf_vectorizer.transform([query])\n",
    "\n",
    "# Find most similar documents\n",
    "similarities = cosine_similarity(query_vector, real_tfidf_matrix)[0]\n",
    "top_doc_indices = np.argsort(similarities)[::-1][:5]\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Most similar documents (TF-IDF):\")\n",
    "for idx in top_doc_indices:\n",
    "    doc_preview = ' '.join(real_sentences[idx][:15]) + '...'\n",
    "    print(f\"\\n[{idx}] Similarity: {similarities[idx]:.4f}\")\n",
    "    print(f\"  {doc_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3: TF-IDF vs Word Embeddings Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: TF-IDF vs Word Embeddings (Word2Vec, FastText, GloVe)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = [\n",
    "    [\"Characteristic\", \"TF-IDF\", \"Word Embeddings\"],\n",
    "    [\"-\" * 20, \"-\" * 30, \"-\" * 30],\n",
    "    [\"Representation\", \"Sparse (mostly zeros)\", \"Dense (all values meaningful)\"],\n",
    "    [\"Dimensionality\", \"High (vocab size)\", \"Low (50-300 typically)\"],\n",
    "    [\"Semantic Meaning\", \"‚ùå No\", \"‚úÖ Yes\"],\n",
    "    [\"Word Similarity\", \"‚ùå Cannot compare words\", \"‚úÖ Can measure similarity\"],\n",
    "    [\"OOV Handling\", \"‚ùå No (except FastText)\", \"‚úÖ FastText handles OOV\"],\n",
    "    [\"Training Required\", \"‚ùå No (count-based)\", \"‚úÖ Yes (learned)\"],\n",
    "    [\"Computation\", \"Fast (simple counting)\", \"Slower (neural networks)\"],\n",
    "    [\"Memory Usage\", \"High (sparse but large)\", \"Low (dense but small)\"],\n",
    "    [\"Best For\", \"Document retrieval, search\", \"Semantic tasks, similarity\"],\n",
    "    [\"Context\", \"Document-level\", \"Word-level context\"],\n",
    "]\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"{row[0]:<20} | {row[1]:<30} | {row[2]:<30}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   TF-IDF: Good for finding documents with specific keywords\")\n",
    "print(\"   Embeddings: Good for understanding meaning and semantic relationships\")\n",
    "print(\"\\n   In practice, both can be used together!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dimensionality difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# TF-IDF vector for one document\n",
    "sample_tfidf = real_tfidf_matrix[0].toarray()[0]\n",
    "axes[0].bar(range(len(sample_tfidf)), sample_tfidf, width=1.0, alpha=0.7)\n",
    "axes[0].set_title(f'TF-IDF Vector (Sparse)\\nDimensions: {len(sample_tfidf)}, Non-zeros: {np.count_nonzero(sample_tfidf)}')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('TF-IDF Weight')\n",
    "axes[0].set_xlim(0, min(200, len(sample_tfidf)))  # Show first 200\n",
    "\n",
    "# Word embedding vector (will be added after training)\n",
    "# Placeholder - will show actual embedding after Word2Vec training\n",
    "axes[1].text(0.5, 0.5, 'Word Embeddings\\n(Dense vectors)\\nWill be shown after training!\\n\\nTypically 50-300 dimensions\\nAll values are meaningful', \n",
    "             ha='center', va='center', fontsize=12, transform=axes[1].transAxes)\n",
    "axes[1].set_title('Word Embedding Vector (Dense)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTF-IDF: {len(sample_tfidf)} dimensions, {(1 - np.count_nonzero(sample_tfidf)/len(sample_tfidf))*100:.1f}% sparse\")\n",
    "print(f\"Word Embeddings: 50-300 dimensions, 0% sparse (all values meaningful)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: Training on Dummy Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Word2Vec CBOW on Dummy Data\n",
    "\n",
    "CBOW (Continuous Bag of Words) predicts the target word from context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW model\n",
    "# sg=0 means CBOW (Continuous Bag of Words)\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=dummy_sentences,\n",
    "    vector_size=50,          # Embedding dimension\n",
    "    window=2,                # Context window size\n",
    "    min_count=1,             # Minimum word frequency\n",
    "    sg=0,                    # 0 = CBOW, 1 = Skip-gram\n",
    "    epochs=100,              # Number of training iterations\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"CBOW Model trained!\")\n",
    "print(f\"Vector size: {cbow_model.vector_size}\")\n",
    "print(f\"Vocabulary size: {len(cbow_model.wv)}\")\n",
    "\n",
    "# Get word vector\n",
    "cat_vector = cbow_model.wv['cat']\n",
    "print(f\"\\nVector for 'cat' (first 10 dims): {cat_vector[:10]}\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "for word, score in cbow_model.wv.most_similar('cat', topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Word2Vec Skip-gram on Dummy Data\n",
    "\n",
    "Skip-gram predicts context words from the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Skip-gram model\n",
    "# sg=1 means Skip-gram\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=dummy_sentences,\n",
    "    vector_size=50,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    sg=1,                    # 1 = Skip-gram\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Skip-gram Model trained!\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nMost similar words to 'cat' (Skip-gram):\")\n",
    "for word, score in skipgram_model.wv.most_similar('cat', topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: GloVe Implementation on Dummy Data\n",
    "\n",
    "GloVe learns embeddings from global co-occurrence statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGloVe:\n",
    "    \"\"\"Simplified GloVe implementation for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_size=50, window_size=2, learning_rate=0.05, epochs=100):\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Build vocabulary from sentences\"\"\"\n",
    "        words = set(word for sentence in sentences for word in sentence)\n",
    "        self.word_to_id = {word: idx for idx, word in enumerate(sorted(words))}\n",
    "        self.id_to_word = {idx: word for word, idx in self.word_to_id.items()}\n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "        \n",
    "    def build_cooccurrence_matrix(self, sentences):\n",
    "        \"\"\"Build co-occurrence matrix\"\"\"\n",
    "        cooccur = defaultdict(float)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in self.word_to_id:\n",
    "                    continue\n",
    "                word_id = self.word_to_id[word]\n",
    "                \n",
    "                # Look at context words within window\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(sentence), i + self.window_size + 1)\n",
    "                \n",
    "                for j in range(start, end):\n",
    "                    if i != j and sentence[j] in self.word_to_id:\n",
    "                        context_id = self.word_to_id[sentence[j]]\n",
    "                        distance = abs(i - j)\n",
    "                        # Weight by distance\n",
    "                        cooccur[(word_id, context_id)] += 1.0 / distance\n",
    "        \n",
    "        return cooccur\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train GloVe embeddings\"\"\"\n",
    "        self.build_vocab(sentences)\n",
    "        cooccur = self.build_cooccurrence_matrix(sentences)\n",
    "        \n",
    "        # Initialize word vectors and biases randomly\n",
    "        np.random.seed(42)\n",
    "        self.W = np.random.randn(self.vocab_size, self.vector_size) * 0.01\n",
    "        self.W_context = np.random.randn(self.vocab_size, self.vector_size) * 0.01\n",
    "        self.b = np.zeros(self.vocab_size)\n",
    "        self.b_context = np.zeros(self.vocab_size)\n",
    "        \n",
    "        # Training loop\n",
    "        print(f\"Training GloVe on {len(cooccur)} co-occurrence pairs...\")\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for (i, j), X_ij in cooccur.items():\n",
    "                # Compute weighted loss\n",
    "                weight = min((X_ij / 100.0) ** 0.75, 1.0)  # x_max = 100, alpha = 0.75\n",
    "                \n",
    "                # Compute dot product\n",
    "                diff = (self.W[i] @ self.W_context[j] + \n",
    "                       self.b[i] + self.b_context[j] - np.log(X_ij))\n",
    "                \n",
    "                loss = weight * (diff ** 2)\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Gradients\n",
    "                grad = weight * diff\n",
    "                \n",
    "                # Update parameters\n",
    "                self.W[i] -= self.learning_rate * grad * self.W_context[j]\n",
    "                self.W_context[j] -= self.learning_rate * grad * self.W[i]\n",
    "                self.b[i] -= self.learning_rate * grad\n",
    "                self.b_context[j] -= self.learning_rate * grad\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{self.epochs}, Loss: {total_loss:.4f}\")\n",
    "        \n",
    "        # Final embeddings: sum of word and context vectors\n",
    "        self.embeddings = self.W + self.W_context\n",
    "        \n",
    "    def get_vector(self, word):\n",
    "        \"\"\"Get embedding for a word\"\"\"\n",
    "        if word in self.word_to_id:\n",
    "            return self.embeddings[self.word_to_id[word]]\n",
    "        return None\n",
    "    \n",
    "    def most_similar(self, word, topn=5):\n",
    "        \"\"\"Find most similar words\"\"\"\n",
    "        if word not in self.word_to_id:\n",
    "            return []\n",
    "        \n",
    "        word_vec = self.get_vector(word).reshape(1, -1)\n",
    "        similarities = cosine_similarity(word_vec, self.embeddings)[0]\n",
    "        \n",
    "        # Get top similar words (excluding the word itself)\n",
    "        word_id = self.word_to_id[word]\n",
    "        similarities[word_id] = -1  # Exclude self\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[-topn:][::-1]\n",
    "        return [(self.id_to_word[idx], similarities[idx]) for idx in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GloVe model\n",
    "glove_model = SimpleGloVe(vector_size=50, window_size=2, epochs=100)\n",
    "glove_model.train(dummy_sentences)\n",
    "\n",
    "print(\"\\nMost similar words to 'cat' (GloVe):\")\n",
    "for word, score in glove_model.most_similar('cat', topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4: FastText on Dummy Data\n",
    "\n",
    "FastText extends Word2Vec by representing words as bags of character n-grams.\n",
    "This helps with:\n",
    "- Handling out-of-vocabulary (OOV) words\n",
    "- Capturing morphological information\n",
    "- Better representations for rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Train FastText model (CBOW)\n",
    "fasttext_cbow = FastText(\n",
    "    sentences=dummy_sentences,\n",
    "    vector_size=50,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    sg=0,                    # 0 = CBOW, 1 = Skip-gram\n",
    "    min_n=2,                 # Minimum character n-gram length\n",
    "    max_n=5,                 # Maximum character n-gram length\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"FastText CBOW Model trained!\")\n",
    "print(f\"Vector size: {fasttext_cbow.vector_size}\")\n",
    "print(f\"Vocabulary size: {len(fasttext_cbow.wv)}\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nMost similar words to 'cat' (FastText CBOW):\")\n",
    "for word, score in fasttext_cbow.wv.most_similar('cat', topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Test OOV word handling - FastText can generate vectors for unseen words!\n",
    "print(\"\\nTesting OOV handling:\")\n",
    "oov_word = 'catty'  # Not in training corpus\n",
    "if oov_word not in fasttext_cbow.wv:\n",
    "    print(f\"'{oov_word}' is not in vocabulary, but FastText can still generate a vector!\")\n",
    "    oov_vector = fasttext_cbow.wv[oov_word]\n",
    "    print(f\"Vector for '{oov_word}' (first 10 dims): {oov_vector[:10]}\")\n",
    "    \n",
    "    # Find similar words to OOV word\n",
    "    print(f\"\\nMost similar words to '{oov_word}':\")\n",
    "    for word, score in fasttext_cbow.wv.most_similar(oov_word, topn=5):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FastText Skip-gram model\n",
    "fasttext_skipgram = FastText(\n",
    "    sentences=dummy_sentences,\n",
    "    vector_size=50,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    sg=1,                    # Skip-gram\n",
    "    min_n=2,\n",
    "    max_n=5,\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"FastText Skip-gram Model trained!\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nMost similar words to 'cat' (FastText Skip-gram):\")\n",
    "for word, score in fasttext_skipgram.wv.most_similar('cat', topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: Training on Real Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Word2Vec CBOW on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_cbow = Word2Vec(\n",
    "    sentences=real_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,           # Ignore rare words\n",
    "    sg=0,                  # CBOW\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"CBOW Model trained on real data!\")\n",
    "print(f\"Vocabulary size: {len(real_cbow.wv)}\")\n",
    "\n",
    "# Test word analogies and similarities\n",
    "test_words = ['business', 'company', 'market', 'government', 'sports']\n",
    "available_test_words = [w for w in test_words if w in real_cbow.wv]\n",
    "\n",
    "if available_test_words:\n",
    "    test_word = available_test_words[0]\n",
    "    print(f\"\\nMost similar words to '{test_word}':\")\n",
    "    for word, score in real_cbow.wv.most_similar(test_word, topn=5):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Word2Vec Skip-gram on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_skipgram = Word2Vec(\n",
    "    sentences=real_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,                  # Skip-gram\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Skip-gram Model trained on real data!\")\n",
    "print(f\"Vocabulary size: {len(real_skipgram.wv)}\")\n",
    "\n",
    "if available_test_words:\n",
    "    test_word = available_test_words[0]\n",
    "    print(f\"\\nMost similar words to '{test_word}' (Skip-gram):\")\n",
    "    for word, score in real_skipgram.wv.most_similar(test_word, topn=5):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: GloVe on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GloVe on real data\n",
    "print(\"Training GloVe on real dataset...\")\n",
    "real_glove = SimpleGloVe(\n",
    "    vector_size=100,\n",
    "    window_size=5,\n",
    "    learning_rate=0.05,\n",
    "    epochs=50  # Fewer epochs for larger dataset\n",
    ")\n",
    "\n",
    "# Note: Training GloVe on large corpus takes time\n",
    "# For demonstration, we'll use a subset\n",
    "glove_subset = real_sentences[:1000]  # Use first 1000 sentences\n",
    "print(f\"Training on {len(glove_subset)} sentences...\")\n",
    "real_glove.train(glove_subset)\n",
    "\n",
    "print(\"\\nGloVe Model trained on real data!\")\n",
    "print(f\"Vocabulary size: {real_glove.vocab_size}\")\n",
    "\n",
    "# Test the model\n",
    "test_words_glove = ['business', 'company', 'market', 'government', 'sports', 'the', 'is', 'to']\n",
    "available_glove_words = [w for w in test_words_glove if w in real_glove.word_to_id]\n",
    "\n",
    "if available_glove_words:\n",
    "    test_word = available_glove_words[0]\n",
    "    print(f\"\\nMost similar words to '{test_word}' (GloVe):\")\n",
    "    for word, score in real_glove.most_similar(test_word, topn=5):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: FastText on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FastText CBOW on real data\n",
    "real_fasttext_cbow = FastText(\n",
    "    sentences=real_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0,                    # CBOW\n",
    "    min_n=3,                 # Character n-grams from 3 to 6\n",
    "    max_n=6,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"FastText CBOW Model trained on real data!\")\n",
    "print(f\"Vocabulary size: {len(real_fasttext_cbow.wv)}\")\n",
    "\n",
    "if available_test_words:\n",
    "    test_word = available_test_words[0]\n",
    "    print(f\"\\nMost similar words to '{test_word}' (FastText):\")\n",
    "    for word, score in real_fasttext_cbow.wv.most_similar(test_word, topn=5):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FastText Skip-gram on real data\n",
    "real_fasttext_skipgram = FastText(\n",
    "    sentences=real_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,                    # Skip-gram\n",
    "    min_n=3,\n",
    "    max_n=6,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"FastText Skip-gram Model trained on real data!\")\n",
    "print(f\"Vocabulary size: {len(real_fasttext_skipgram.wv)}\")\n",
    "\n",
    "if available_test_words:\n",
    "    test_word = available_test_words[0]\n",
    "    print(f\"\\nMost similar words to '{test_word}' (FastText Skip-gram):\")\n",
    "    for word, score in real_fasttext_skipgram.wv.most_similar(test_word, topn=5):\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing FastText's OOV Handling on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate FastText's ability to handle OOV words\n",
    "print(\"Testing Out-of-Vocabulary (OOV) word handling:\\n\")\n",
    "\n",
    "# Create some OOV words based on known words\n",
    "if 'business' in real_fasttext_cbow.wv:\n",
    "    oov_tests = ['businesses', 'businessman', 'businesslike', 'unbusiness']\n",
    "    known_word = 'business'\n",
    "elif 'company' in real_fasttext_cbow.wv:\n",
    "    oov_tests = ['companies', 'accompanying', 'companionship']\n",
    "    known_word = 'company'\n",
    "else:\n",
    "    oov_tests = ['testing', 'tested', 'tester']\n",
    "    known_word = 'test'\n",
    "\n",
    "print(f\"Base word: '{known_word}'\\n\")\n",
    "\n",
    "for oov_word in oov_tests:\n",
    "    if oov_word not in real_fasttext_cbow.wv:\n",
    "        print(f\"OOV Word: '{oov_word}'\")\n",
    "        # FastText can still generate vectors for these!\n",
    "        similar = real_fasttext_cbow.wv.most_similar(oov_word, topn=3)\n",
    "        print(f\"  Most similar words:\")\n",
    "        for word, score in similar:\n",
    "            print(f\"    {word}: {score:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Compare with Word2Vec (which cannot handle OOV)\n",
    "print(\"\\nComparison: Word2Vec cannot handle OOV words\")\n",
    "oov_word = oov_tests[0]\n",
    "try:\n",
    "    if oov_word not in real_cbow.wv:\n",
    "        _ = real_cbow.wv[oov_word]\n",
    "except KeyError:\n",
    "    print(f\"Word2Vec: Cannot generate vector for '{oov_word}' (KeyError)\")\n",
    "\n",
    "print(f\"FastText: Can generate vector for '{oov_word}' using character n-grams!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vs Embeddings: Practical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare document similarity using TF-IDF vs Word Embeddings\n",
    "def document_vector_from_embeddings(doc_text, model):\n",
    "    \"\"\"Create document vector by averaging word embeddings\"\"\"\n",
    "    words = doc_text.lower().split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if hasattr(model, 'wv') and word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    return None\n",
    "\n",
    "# Test query\n",
    "test_query = \"financial market business\"\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TF-IDF search\n",
    "print(\"\\nTF-IDF Results (keyword matching):\")\n",
    "query_tfidf = real_tfidf_vectorizer.transform([test_query])\n",
    "tfidf_sims = cosine_similarity(query_tfidf, real_tfidf_matrix)[0]\n",
    "top_tfidf = np.argsort(tfidf_sims)[::-1][:3]\n",
    "\n",
    "for idx in top_tfidf:\n",
    "    preview = ' '.join(real_sentences[idx][:12]) + '...'\n",
    "    print(f\"  [{idx}] Score: {tfidf_sims[idx]:.4f}\")\n",
    "    print(f\"       {preview}\\n\")\n",
    "\n",
    "# Word2Vec search\n",
    "print(\"\\nWord2Vec Results (semantic matching):\")\n",
    "query_w2v = document_vector_from_embeddings(test_query, real_cbow)\n",
    "if query_w2v is not None:\n",
    "    w2v_sims = []\n",
    "    for sent in real_sentences:\n",
    "        sent_vec = document_vector_from_embeddings(' '.join(sent), real_cbow)\n",
    "        if sent_vec is not None:\n",
    "            sim = cosine_similarity([query_w2v], [sent_vec])[0][0]\n",
    "            w2v_sims.append(sim)\n",
    "        else:\n",
    "            w2v_sims.append(-1)\n",
    "    \n",
    "    top_w2v = np.argsort(w2v_sims)[::-1][:3]\n",
    "    for idx in top_w2v:\n",
    "        preview = ' '.join(real_sentences[idx][:12]) + '...'\n",
    "        print(f\"  [{idx}] Score: {w2v_sims[idx]:.4f}\")\n",
    "        print(f\"       {preview}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"   - TF-IDF: Finds documents with exact keyword matches\")\n",
    "print(\"   - Word2Vec: Finds documents with semantic similarity (synonyms, related concepts)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison: Storage requirements\n",
    "import sys\n",
    "\n",
    "print(\"\\nSTORAGE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# TF-IDF storage\n",
    "tfidf_size = real_tfidf_matrix.data.nbytes + real_tfidf_matrix.indices.nbytes + real_tfidf_matrix.indptr.nbytes\n",
    "print(f\"TF-IDF Matrix: {tfidf_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  - Shape: {real_tfidf_matrix.shape}\")\n",
    "print(f\"  - Sparsity: {(1 - real_tfidf_matrix.nnz / (real_tfidf_matrix.shape[0] * real_tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Word2Vec storage\n",
    "w2v_size = real_cbow.wv.vectors.nbytes\n",
    "print(f\"\\nWord2Vec Embeddings: {w2v_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  - Vocabulary: {len(real_cbow.wv)}\")\n",
    "print(f\"  - Dimensions: {real_cbow.vector_size}\")\n",
    "\n",
    "# FastText storage\n",
    "ft_size = real_fasttext_cbow.wv.vectors.nbytes + real_fasttext_cbow.wv.vectors_ngrams.nbytes\n",
    "print(f\"\\nFastText Embeddings: {ft_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  - Vocabulary: {len(real_fasttext_cbow.wv)}\")\n",
    "print(f\"  - Dimensions: {real_fasttext_cbow.vector_size}\")\n",
    "print(f\"  - N-grams: {len(real_fasttext_cbow.wv.vectors_ngrams)} additional vectors\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key Points:\")\n",
    "print(\"   - TF-IDF grows with number of documents (one vector per document)\")\n",
    "print(\"   - Embeddings grow with vocabulary size (one vector per word)\")\n",
    "print(\"   - For large corpora, embeddings are usually more efficient\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: Visualization and Comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, words, title):\n",
    "    \"\"\"Visualize word embeddings using PCA\"\"\"\n",
    "    # Get vectors for words\n",
    "    vectors = []\n",
    "    valid_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if hasattr(model, 'wv') and word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "            valid_words.append(word)\n",
    "        elif hasattr(model, 'get_vector') and model.get_vector(word) is not None:\n",
    "            vectors.append(model.get_vector(word))\n",
    "            valid_words.append(word)\n",
    "    \n",
    "    if len(vectors) < 2:\n",
    "        print(f\"Not enough valid words for visualization in {title}\")\n",
    "        return\n",
    "    \n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    vectors_2d = pca.fit_transform(vectors)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=100, alpha=0.6)\n",
    "    \n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dummy data models\n",
    "dummy_words = ['cat', 'dog', 'pet', 'animal', 'mat', 'sat']\n",
    "print(\"Visualizing embeddings from dummy data...\")\n",
    "visualize_embeddings(cbow_model, dummy_words, \"CBOW Embeddings (Dummy Data)\")\n",
    "visualize_embeddings(skipgram_model, dummy_words, \"Skip-gram Embeddings (Dummy Data)\")\n",
    "visualize_embeddings(fasttext_cbow, dummy_words, \"FastText CBOW Embeddings (Dummy Data)\")\n",
    "visualize_embeddings(fasttext_skipgram, dummy_words, \"FastText Skip-gram Embeddings (Dummy Data)\")\n",
    "visualize_embeddings(glove_model, dummy_words, \"GloVe Embeddings (Dummy Data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real data models\n",
    "if available_test_words:\n",
    "    real_words = available_test_words[:10] if len(available_test_words) >= 10 else available_test_words\n",
    "    print(\"Visualizing embeddings from real data...\")\n",
    "    visualize_embeddings(real_cbow, real_words, \"CBOW Embeddings (AG News)\")\n",
    "    visualize_embeddings(real_skipgram, real_words, \"Skip-gram Embeddings (AG News)\")\n",
    "    visualize_embeddings(real_fasttext_cbow, real_words, \"FastText CBOW Embeddings (AG News)\")\n",
    "    visualize_embeddings(real_fasttext_skipgram, real_words, \"FastText Skip-gram Embeddings (AG News)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GloVe embeddings on real data\n",
    "if available_glove_words:\n",
    "    glove_viz_words = available_glove_words[:10] if len(available_glove_words) >= 10 else available_glove_words\n",
    "    print(\"Visualizing GloVe embeddings on real data...\")\n",
    "    visualize_embeddings(real_glove, glove_viz_words, \"GloVe Embeddings (AG News Subset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: Model Comparison: Word2Vec vs FastText vs GloVe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEY DIFFERENCES (from lecture slides):\n",
    "\n",
    "### CBOW (Continuous Bag of Words):\n",
    "- Predicts target word from context words\n",
    "- Faster to train\n",
    "- Better for frequent words\n",
    "- Smooths over distributional information\n",
    "- Local context window approach\n",
    "\n",
    "### Skip-gram:\n",
    "- Predicts context words from target word\n",
    "- Works better with rare words\n",
    "- Better for small datasets\n",
    "- Captures more detailed word relationships\n",
    "- Local context window approach\n",
    "\n",
    "### FastText:\n",
    "- Extension of Word2Vec using character n-grams\n",
    "- Can handle out-of-vocabulary (OOV) words\n",
    "- Captures morphological information (prefixes, suffixes)\n",
    "- Better for morphologically rich languages\n",
    "- Slightly slower to train but more robust\n",
    "- Works with both CBOW and Skip-gram architectures\n",
    "\n",
    "### GloVe (Global Vectors):\n",
    "- Uses global co-occurrence statistics\n",
    "- Learns from word-word co-occurrence matrix\n",
    "- Combines benefits of matrix factorization and local context\n",
    "- Often produces high-quality embeddings\n",
    "- Deterministic training (given same data)\n",
    "- Can be slower to train on very large corpora\n",
    "\n",
    "### Comparison Summary:\n",
    "\n",
    "| Feature | Word2Vec | FastText | GloVe |\n",
    "|---------|----------|----------|-------|\n",
    "| OOV Handling | ‚ùå No | ‚úÖ Yes | ‚ùå No |\n",
    "| Training Speed | Fast | Medium | Slow |\n",
    "| Morphology | ‚ùå No | ‚úÖ Yes | ‚ùå No |\n",
    "| Context Type | Local | Local | Global |\n",
    "| Rare Words | Skip-gram better | Excellent | Good |\n",
    "| Memory Usage | Low | Medium | High |\n",
    "\n",
    "### In this notebook:\n",
    "- Dummy data: All models work similarly on small vocabulary\n",
    "- Real data: \n",
    "  - FastText excels at handling OOV and morphological variations\n",
    "  - Skip-gram often captures more nuanced relationships\n",
    "  - GloVe captures global corpus statistics\n",
    "  - FastText provides most robust representations for varied vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Quantitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models on common words\n",
    "def compare_models_similarity(word, models_dict):\n",
    "    \"\"\"Compare similarity results across different models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Comparing models for word: '{word}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        try:\n",
    "            if hasattr(model, 'wv'):\n",
    "                if word in model.wv:\n",
    "                    similar = model.wv.most_similar(word, topn=3)\n",
    "                    for w, score in similar:\n",
    "                        print(f\"  {w}: {score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  Word '{word}' not in vocabulary\")\n",
    "            elif hasattr(model, 'most_similar'):\n",
    "                similar = model.most_similar(word, topn=3)\n",
    "                for w, score in similar:\n",
    "                    print(f\"  {w}: {score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "        print()\n",
    "\n",
    "# Compare on dummy data\n",
    "dummy_models = {\n",
    "    \"Word2Vec CBOW\": cbow_model,\n",
    "    \"Word2Vec Skip-gram\": skipgram_model,\n",
    "    \"FastText CBOW\": fasttext_cbow,\n",
    "    \"FastText Skip-gram\": fasttext_skipgram,\n",
    "    \"GloVe\": glove_model\n",
    "}\n",
    "\n",
    "compare_models_similarity('cat', dummy_models)\n",
    "compare_models_similarity('dog', dummy_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on real data\n",
    "if available_test_words:\n",
    "    real_models = {\n",
    "        \"Word2Vec CBOW\": real_cbow,\n",
    "        \"Word2Vec Skip-gram\": real_skipgram,\n",
    "        \"FastText CBOW\": real_fasttext_cbow,\n",
    "        \"FastText Skip-gram\": real_fasttext_skipgram,\n",
    "        \"GloVe\": real_glove\n",
    "    }\n",
    "    \n",
    "    test_word = available_test_words[0]\n",
    "    compare_models_similarity(test_word, real_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OOV handling comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OOV WORD HANDLING COMPARISON\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "oov_word = 'businessman'  # Likely not in vocabulary\n",
    "print(f\"Testing OOV word: '{oov_word}'\\n\")\n",
    "\n",
    "print(\"Word2Vec CBOW:\")\n",
    "try:\n",
    "    if oov_word not in real_cbow.wv:\n",
    "        _ = real_cbow.wv[oov_word]\n",
    "except KeyError:\n",
    "    print(\"  ‚ùå Cannot handle OOV word (KeyError)\\n\")\n",
    "\n",
    "print(\"FastText CBOW:\")\n",
    "try:\n",
    "    vec = real_fasttext_cbow.wv[oov_word]\n",
    "    print(\"  ‚úÖ Can generate vector using character n-grams\")\n",
    "    similar = real_fasttext_cbow.wv.most_similar(oov_word, topn=3)\n",
    "    print(\"  Most similar:\")\n",
    "    for w, score in similar:\n",
    "        print(f\"    {w}: {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "print()\n",
    "\n",
    "print(\"GloVe:\")\n",
    "if oov_word not in real_glove.word_to_id:\n",
    "    print(\"  ‚ùå Cannot handle OOV word (not in vocabulary)\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Word is in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: Saving and Loading Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "print(\"Saving models...\")\n",
    "real_cbow.save(\"word2vec_cbow.model\")\n",
    "real_skipgram.save(\"word2vec_skipgram.model\")\n",
    "real_fasttext_cbow.save(\"fasttext_cbow.model\")\n",
    "real_fasttext_skipgram.save(\"fasttext_skipgram.model\")\n",
    "\n",
    "# Save GloVe embeddings (save as numpy arrays)\n",
    "np.save(\"glove_embeddings.npy\", real_glove.embeddings)\n",
    "\n",
    "# Save vocabulary mappings\n",
    "import pickle\n",
    "with open(\"glove_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        'word_to_id': real_glove.word_to_id,\n",
    "        'id_to_word': real_glove.id_to_word\n",
    "    }, f)\n",
    "\n",
    "print(\"All models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "loaded_cbow = Word2Vec.load(\"word2vec_cbow.model\")\n",
    "loaded_fasttext = FastText.load(\"fasttext_cbow.model\")\n",
    "\n",
    "# Load GloVe\n",
    "import pickle\n",
    "loaded_glove = SimpleGloVe(vector_size=100)\n",
    "loaded_glove.embeddings = np.load(\"glove_embeddings.npy\")\n",
    "with open(\"glove_vocab.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "    loaded_glove.word_to_id = vocab_data['word_to_id']\n",
    "    loaded_glove.id_to_word = vocab_data['id_to_word']\n",
    "    loaded_glove.vocab_size = len(loaded_glove.word_to_id)\n",
    "\n",
    "print(\"All models loaded successfully!\")\n",
    "\n",
    "# Test loaded models\n",
    "if available_test_words:\n",
    "    test_word = available_test_words[0]\n",
    "    print(f\"\\nTesting loaded models with '{test_word}':\")\n",
    "    \n",
    "    print(\"\\nWord2Vec CBOW:\")\n",
    "    for word, score in loaded_cbow.wv.most_similar(test_word, topn=3):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\nFastText CBOW:\")\n",
    "    for word, score in loaded_fasttext.wv.most_similar(test_word, topn=3):\n",
    "        print(f\"  {word}: {score:.4f}\")\n",
    "    \n",
    "    if test_word in loaded_glove.word_to_id:\n",
    "        print(\"\\nGloVe:\")\n",
    "        for word, score in loaded_glove.most_similar(test_word, topn=3):\n",
    "            print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: When to Use Each Model\n",
    "\n",
    "**Use Word2Vec when:**\n",
    "- You have clean, well-formed text\n",
    "- Vocabulary is fixed and known\n",
    "- Training speed is important\n",
    "- Memory is limited\n",
    "\n",
    "**Use FastText when:**\n",
    "- Text contains typos or misspellings\n",
    "- Working with morphologically rich languages\n",
    "- Need to handle OOV words in production\n",
    "- Working with user-generated content (social media, etc.)\n",
    "\n",
    "**Use GloVe when:**\n",
    "- You want to leverage global corpus statistics\n",
    "- Training can be done offline once\n",
    "- You have sufficient computational resources\n",
    "- Deterministic results are important\n",
    "\n",
    "**In Practice:**\n",
    "- For most applications: Start with FastText (most robust)\n",
    "- For research/analysis: Try all three and compare\n",
    "- For production: Consider pre-trained embeddings first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXERCISES FOR STUDENTS\n",
    "---\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "## Basic Exercises (Beginner)\n",
    "\n",
    "### 1. Experiment with hyperparameters:\n",
    "- Change `vector_size` (50, 100, 200, 300) - How does dimensionality affect quality?\n",
    "- Change `window` size (2, 5, 10, 15) - How does context size impact learned relationships?\n",
    "- For FastText: try different n-gram ranges:\n",
    "  - `min_n=2, max_n=3` (short n-grams)\n",
    "  - `min_n=3, max_n=6` (balanced)\n",
    "  - `min_n=4, max_n=8` (long n-grams)\n",
    "- For GloVe: experiment with `learning_rate` (0.01, 0.05, 0.1) and `epochs` (50, 100, 200)\n",
    "- **Question:** Which settings give the best word similarities?\n",
    "\n",
    "### 2. Explore word similarities:\n",
    "- Pick different words and find their most similar neighbors\n",
    "- Compare results across all models (Word2Vec, FastText, GloVe)\n",
    "- Try words from different categories:\n",
    "  - Common words: 'the', 'is', 'and'\n",
    "  - Nouns: 'business', 'market', 'company'\n",
    "  - Verbs: 'run', 'jump', 'think'\n",
    "  - Adjectives: 'good', 'bad', 'beautiful'\n",
    "- **Question:** Do different models capture different types of relationships?\n",
    "\n",
    "### 3. Visualize semantic clusters:\n",
    "- Create groups of related words:\n",
    "  - Animals: ['cat', 'dog', 'bird', 'fish', 'lion', 'elephant']\n",
    "  - Colors: ['red', 'blue', 'green', 'yellow', 'black', 'white']\n",
    "  - Emotions: ['happy', 'sad', 'angry', 'excited', 'worried']\n",
    "  - Numbers: ['one', 'two', 'three', 'four', 'five']\n",
    "- Visualize each cluster using PCA or t-SNE\n",
    "- **Question:** Do semantically related words cluster together?\n",
    "\n",
    "## Intermediate Exercises\n",
    "\n",
    "### 4. Test word analogies:\n",
    "- Implement the classic analogy task: A is to B as C is to ?\n",
    "- Examples to try:\n",
    "  - king - man + woman ‚âà queen\n",
    "  - paris - france + germany ‚âà berlin\n",
    "  - big - bigger + small ‚âà smaller\n",
    "  - good - better + bad ‚âà worse\n",
    "- Code:\n",
    "```python\n",
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(f\"king - man + woman = {result[0][0]}\")\n",
    "```\n",
    "- **Question:** Which model performs best on analogies? Why?\n",
    "\n",
    "### 5. Out-of-Vocabulary (OOV) stress testing:\n",
    "- Create challenging OOV words:\n",
    "  - Misspellings: 'buisness', 'recieve', 'occured'\n",
    "  - Compound words: 'mega-corporation', 'cyber-attack'\n",
    "  - Informal text: 'looool', 'yasss', 'omgggg'\n",
    "  - Domain-specific: 'COVID-19', 'blockchain', 'AI-powered'\n",
    "- Test with Word2Vec (should fail) vs FastText (should work)\n",
    "- **Question:** How well does FastText capture meaning of OOV words?\n",
    "\n",
    "### 6. Compare CBOW vs Skip-gram:\n",
    "- Train both architectures on the same data\n",
    "- Compare their performance on:\n",
    "  - Frequent words (e.g., 'the', 'is', 'business')\n",
    "  - Rare words (words appearing 5-10 times)\n",
    "  - Word analogies\n",
    "- Measure training time for both\n",
    "- **Question:** When does Skip-gram outperform CBOW?\n",
    "\n",
    "### 7. Load different datasets:\n",
    "Try training on different HuggingFace datasets:\n",
    "```python\n",
    "# Movie reviews\n",
    "dataset = load_dataset('imdb', split='train[:5000]')\n",
    "\n",
    "# Restaurant reviews\n",
    "dataset = load_dataset('yelp_review_full', split='train[:5000]')\n",
    "\n",
    "# Wikipedia articles (small version)\n",
    "dataset = load_dataset('wikipedia', '20220301.simple', split='train[:1000]')\n",
    "\n",
    "# Scientific papers\n",
    "dataset = load_dataset('scientific_papers', 'arxiv', split='train[:1000]')\n",
    "```\n",
    "- **Question:** How does domain affect embedding quality?\n",
    "\n",
    "## Advanced Exercises\n",
    "\n",
    "### 8. Implement embedding evaluation:\n",
    "Create a word similarity evaluation:\n",
    "```python\n",
    "# Define word pairs with human similarity scores (0-10)\n",
    "word_pairs = [\n",
    "    ('cat', 'dog', 8.0),\n",
    "    ('cat', 'car', 1.0),\n",
    "    ('king', 'queen', 9.0),\n",
    "    ('man', 'woman', 8.0),\n",
    "    ('computer', 'keyboard', 7.0),\n",
    "]\n",
    "\n",
    "# Compute correlation between model scores and human scores\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def evaluate_model(model, pairs):\n",
    "    human_scores = []\n",
    "    model_scores = []\n",
    "    for w1, w2, human_score in pairs:\n",
    "        if w1 in model.wv and w2 in model.wv:\n",
    "            model_score = model.wv.similarity(w1, w2)\n",
    "            human_scores.append(human_score)\n",
    "            model_scores.append(model_score * 10)  # Scale to 0-10\n",
    "    \n",
    "    correlation, pvalue = spearmanr(human_scores, model_scores)\n",
    "    return correlation\n",
    "```\n",
    "- **Question:** Which model correlates best with human judgments?\n",
    "\n",
    "### 9. Morphological analysis with FastText:\n",
    "- Test FastText on morphologically related words:\n",
    "  - Verbs: run ‚Üí running ‚Üí runner ‚Üí runs ‚Üí ran\n",
    "  - Adjectives: happy ‚Üí happier ‚Üí happiest ‚Üí happiness ‚Üí unhappy\n",
    "  - Nouns: nation ‚Üí national ‚Üí nationality ‚Üí nationalize\n",
    "- Compute similarity between base word and derivatives\n",
    "- Visualize embeddings of morphological families\n",
    "- **Question:** Does FastText capture morphological relationships?\n",
    "\n",
    "### 10. Visualize with t-SNE:\n",
    "Replace PCA with t-SNE for better 2D visualization:\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_tsne(model, words, title):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    valid_words = [word for word in words if word in model.wv]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(vectors)-1))\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=100, alpha=0.6)\n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "```\n",
    "- **Question:** Does t-SNE reveal different patterns than PCA?\n",
    "\n",
    "### 11. Compare with pre-trained embeddings:\n",
    "Load and compare with pre-trained models:\n",
    "```python\n",
    "# Load pre-trained Word2Vec (Google News)\n",
    "import gensim.downloader as api\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load pre-trained GloVe\n",
    "pretrained_glove = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "# Load pre-trained FastText\n",
    "pretrained_ft = api.load('fasttext-wiki-news-subwords-300')\n",
    "```\n",
    "- Compare with your trained models\n",
    "- **Question:** How much data is needed to match pre-trained quality?\n",
    "\n",
    "### 12. Bias detection in embeddings:\n",
    "Investigate social biases in word embeddings:\n",
    "```python\n",
    "# Gender bias\n",
    "def compute_bias(model, word, male_word='man', female_word='woman'):\n",
    "    male_sim = model.wv.similarity(word, male_word)\n",
    "    female_sim = model.wv.similarity(word, female_word)\n",
    "    return male_sim - female_sim\n",
    "\n",
    "# Test occupations\n",
    "occupations = ['doctor', 'nurse', 'engineer', 'teacher', 'programmer']\n",
    "for occupation in occupations:\n",
    "    if occupation in model.wv:\n",
    "        bias = compute_bias(model, occupation)\n",
    "        print(f\"{occupation}: {bias:.4f} ({'male' if bias > 0 else 'female'} leaning)\")\n",
    "```\n",
    "- **Question:** What biases exist in your embeddings?\n",
    "\n",
    "### 13. Build a semantic search engine:\n",
    "```python\n",
    "def semantic_search(query, documents, model, top_k=5):\n",
    "    # Compute document vectors (average word vectors)\n",
    "    def doc_vector(text):\n",
    "        words = text.lower().split()\n",
    "        vectors = [model.wv[w] for w in words if w in model.wv]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return None\n",
    "    \n",
    "    query_vec = doc_vector(query)\n",
    "    if query_vec is None:\n",
    "        return []\n",
    "    \n",
    "    # Compute similarities\n",
    "    scores = []\n",
    "    for doc in documents:\n",
    "        doc_vec = doc_vector(doc)\n",
    "        if doc_vec is not None:\n",
    "            sim = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "            scores.append((doc, sim))\n",
    "    \n",
    "    # Return top-k\n",
    "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "# Test\n",
    "query = \"business news\"\n",
    "results = semantic_search(query, real_corpus[:100], real_cbow)\n",
    "```\n",
    "- **Question:** How does embedding quality affect search results?\n",
    "\n",
    "## Challenge Exercises\n",
    "\n",
    "### 14. Implement GloVe improvements:\n",
    "- Add adaptive learning rate\n",
    "- Implement mini-batch training\n",
    "- Add different weighting functions\n",
    "- Compare with baseline GloVe\n",
    "\n",
    "### 15. Cross-lingual embeddings:\n",
    "- Train embeddings on multilingual text\n",
    "- Try to align embeddings from different languages\n",
    "- Test translation capabilities\n",
    "\n",
    "### 16. Dynamic embeddings:\n",
    "- Implement a system that updates embeddings as new text arrives\n",
    "- Compare static vs dynamic embedding quality\n",
    "- Handle vocabulary growth\n",
    "\n",
    "### 17. Contextualized embeddings comparison:\n",
    "- Compare static embeddings (Word2Vec, FastText, GloVe) with contextualized embeddings\n",
    "- Try simple context aggregation methods\n",
    "- Analyze when context matters most\n",
    "\n",
    "### 18. TF-IDF Experiments:\n",
    "- Experiment with TF-IDF parameters:\n",
    "  - `max_df` (0.5, 0.8, 0.95) - Remove common words\n",
    "  - `min_df` (1, 5, 10) - Remove rare words\n",
    "  - `max_features` (100, 1000, 5000) - Limit vocabulary\n",
    "  - `ngram_range` - Try bigrams (1,2) or trigrams (1,3)\n",
    "- **Question:** How do these parameters affect retrieval quality?\n",
    "\n",
    "### 19. Hybrid TF-IDF + Embeddings:\n",
    "Combine both approaches:\n",
    "```python\n",
    "def hybrid_similarity(query, documents, tfidf_model, embedding_model, alpha=0.5):\n",
    "    # TF-IDF scores\n",
    "    query_tfidf = tfidf_model.transform([query])\n",
    "    tfidf_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
    "    \n",
    "    # Embedding scores\n",
    "    query_emb = document_vector_from_embeddings(query, embedding_model)\n",
    "    emb_scores = [...] # compute embedding similarities\n",
    "    \n",
    "    # Combine with weight alpha\n",
    "    combined_scores = alpha * tfidf_scores + (1 - alpha) * emb_scores\n",
    "    return combined_scores\n",
    "```\n",
    "- Test different alpha values (0.0 to 1.0)\n",
    "- **Question:** Does combining approaches improve results?\n",
    "\n",
    "### 20. TF-IDF for Feature Engineering:\n",
    "- Use TF-IDF features in a classifier:\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train classifier on TF-IDF features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(tfidf_train, labels_train)\n",
    "```\n",
    "- Compare with embedding-based features\n",
    "- **Question:** When does TF-IDF outperform embeddings for classification?\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "After completing exercises, consider:\n",
    "\n",
    "1. **Data size:** How much data is needed for quality embeddings?\n",
    "2. **Architecture choice:** When should you choose CBOW vs Skip-gram vs FastText vs GloVe?\n",
    "3. **Hyperparameters:** Which hyperparameters have the biggest impact?\n",
    "4. **Evaluation:** How do you measure embedding quality in practice?\n",
    "5. **Applications:** What downstream tasks benefit most from which embeddings?\n",
    "6. **Limitations:** What are the fundamental limitations of static word embeddings?\n",
    "7. **Ethics:** What biases exist in embeddings and how can they be mitigated?\n",
    "8. **Future:** When should you use static embeddings vs transformer-based models?\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Create Your Own Project\n",
    "\n",
    "Choose one:\n",
    "1. **Domain-specific embeddings:** Train embeddings on medical/legal/financial text\n",
    "2. **Embedding-based classifier:** Build a text classifier using embeddings as features\n",
    "3. **Analogy dataset:** Create and evaluate a custom analogy test set\n",
    "4. **Visualization dashboard:** Build an interactive tool to explore embeddings\n",
    "5. **Bias mitigation:** Implement and test a debiasing method\n",
    "6. **Embedding compression:** Reduce embedding dimensions while preserving quality\n",
    "7. **Multi-sense embeddings:** Handle words with multiple meanings\n",
    "8. **Temporal embeddings:** Track how word meanings change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here!\n",
    "# Use this cell to implement your chosen exercises\n",
    "# Good luck! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
