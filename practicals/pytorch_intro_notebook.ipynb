{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch: From Tensors to Training Neural Networks\n",
    "\n",
    "**Based on Sebastian Raschka's PyTorch Tutorial**\n",
    "\n",
    "This notebook will introduce you to the essential concepts of PyTorch in a hands-on, interactive way. By the end of this tutorial, you'll understand:\n",
    "\n",
    "1. What PyTorch is and why it's popular\n",
    "2. Tensors - the fundamental data structure\n",
    "3. Automatic differentiation (autograd)\n",
    "4. Building neural networks\n",
    "5. Training models with a typical training loop\n",
    "6. Working with GPUs\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source Python deep learning library that has become the most widely used framework for  and model development since 2019. It offers the perfect balance between:\n",
    "- **Ease of use**: Intuitive, Python-native API\n",
    "- **Flexibility**: Full control for customization\n",
    "- **Performance**: GPU acceleration for fast training\n",
    "\n",
    "### The Three Core Components of PyTorch:\n",
    "\n",
    "1. **Tensor Library**: Like NumPy but with GPU support\n",
    "2. **Automatic Differentiation (Autograd)**: Computes gradients automatically for backpropagation\n",
    "3. **Deep Learning Library**: Pre-built modules, loss functions, and optimizers\n",
    "\n",
    "Let's start by installing and checking PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (uncomment if needed)\n",
    "# !pip install torch\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"CUDA (GPU) available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch - they're multi-dimensional arrays that can store data:\n",
    "\n",
    "- **Scalar (0D tensor)**: Just a number (e.g., 5)\n",
    "- **Vector (1D tensor)**: Array of numbers (e.g., [1, 2, 3])\n",
    "- **Matrix (2D tensor)**: Table of numbers (e.g., [[1, 2], [3, 4]])\n",
    "- **3D+ tensors**: Higher-dimensional arrays\n",
    "\n",
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scalar (0D tensor)\n",
    "tensor0d = torch.tensor(1)\n",
    "print(f\"Scalar: {tensor0d}\")\n",
    "print(f\"Shape: {tensor0d.shape}\\n\")\n",
    "\n",
    "# Create a vector (1D tensor)\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(f\"Vector: {tensor1d}\")\n",
    "print(f\"Shape: {tensor1d.shape}\\n\")\n",
    "\n",
    "# Create a matrix (2D tensor)\n",
    "tensor2d = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "print(f\"Matrix:\\n{tensor2d}\")\n",
    "print(f\"Shape: {tensor2d.shape}\\n\")\n",
    "\n",
    "# Create a 3D tensor\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], \n",
    "                         [[5, 6], [7, 8]]])\n",
    "print(f\"3D Tensor:\\n{tensor3d}\")\n",
    "print(f\"Shape: {tensor3d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create Your Own Tensors\n",
    "\n",
    "**TODO**: Create the following tensors:\n",
    "1. A scalar with value 42\n",
    "2. A 1D tensor with values [10, 20, 30, 40, 50]\n",
    "3. A 2Ã—2 identity matrix [[1, 0], [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "my_scalar = # TODO\n",
    "my_vector = # TODO\n",
    "my_matrix = # TODO\n",
    "\n",
    "print(f\"My scalar: {my_scalar}\")\n",
    "print(f\"My vector: {my_vector}\")\n",
    "print(f\"My matrix:\\n{my_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Data Types\n",
    "\n",
    "PyTorch tensors have data types (dtypes) that specify how the data is stored:\n",
    "- **Integers**: `torch.int64` (default for Python integers)\n",
    "- **Floats**: `torch.float32` (default for Python floats, most common in deep learning)\n",
    "\n",
    "**Why float32?** It balances precision and computational efficiency, and GPUs are optimized for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "int_tensor = torch.tensor([1, 2, 3])\n",
    "print(f\"Integer tensor dtype: {int_tensor.dtype}\")\n",
    "\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Float tensor dtype: {float_tensor.dtype}\")\n",
    "\n",
    "# Convert data types\n",
    "converted = int_tensor.to(torch.float32)\n",
    "print(f\"Converted dtype: {converted.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Tensor Operations\n",
    "\n",
    "PyTorch provides a NumPy-like API for tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "print(f\"Original tensor:\\n{tensor}\")\n",
    "print(f\"Shape: {tensor.shape}\\n\")\n",
    "\n",
    "# Reshape/View - change dimensions\n",
    "reshaped = tensor.view(3, 2)\n",
    "print(f\"Reshaped (3Ã—2):\\n{reshaped}\\n\")\n",
    "\n",
    "# Transpose - flip across diagonal\n",
    "transposed = tensor.T\n",
    "print(f\"Transposed:\\n{transposed}\\n\")\n",
    "\n",
    "# Matrix multiplication\n",
    "result = tensor @ tensor.T\n",
    "print(f\"Matrix multiplication (tensor @ tensor.T):\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Tensor Operations\n",
    "\n",
    "**TODO**: Given the tensor below:\n",
    "1. Reshape it to shape (4, 3)\n",
    "2. Compute the transpose\n",
    "3. Find the sum of all elements using `.sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_tensor = torch.tensor([[1, 2, 3, 4],\n",
    "                                [5, 6, 7, 8],\n",
    "                                [9, 10, 11, 12]])\n",
    "\n",
    "# Your code here\n",
    "reshaped = # TODO\n",
    "transposed = # TODO\n",
    "total_sum = # TODO\n",
    "\n",
    "print(f\"Reshaped:\\n{reshaped}\\n\")\n",
    "print(f\"Transposed:\\n{transposed}\\n\")\n",
    "print(f\"Sum of all elements: {total_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Computation Graphs and Automatic Differentiation\n",
    "\n",
    "### What is a Computation Graph?\n",
    "\n",
    "A computation graph tracks the sequence of operations needed to compute an output. PyTorch builds this graph automatically to compute gradients during backpropagation.\n",
    "\n",
    "**Example**: Simple logistic regression forward pass\n",
    "```\n",
    "z = x1 * w1 + b     (net input)\n",
    "a = sigmoid(z)      (activation)\n",
    "loss = BCE(a, y)    (binary cross-entropy loss)\n",
    "```\n",
    "\n",
    "### Automatic Differentiation (Autograd)\n",
    "\n",
    "PyTorch's autograd engine automatically computes gradients - no manual calculus needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up a simple computation\n",
    "y = torch.tensor([1.0])  # true label\n",
    "x1 = torch.tensor([1.1])  # input feature\n",
    "\n",
    "# Parameters - requires_grad=True enables gradient tracking\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass - compute gradients automatically!\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradient w.r.t. w1: {w1.grad}\")\n",
    "print(f\"Gradient w.r.t. b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points**:\n",
    "- `requires_grad=True`: Tells PyTorch to track operations for gradient computation\n",
    "- `.backward()`: Computes all gradients automatically\n",
    "- `.grad`: Stores the computed gradient for each tensor\n",
    "\n",
    "This is the magic that makes training neural networks easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Manual Gradient Computation\n",
    "\n",
    "**TODO**: Complete the forward pass and compute gradients\n",
    "1. Compute `z = x * w + b`\n",
    "2. Compute the loss: `loss = (z - target)**2` (mean squared error)\n",
    "3. Call `.backward()` to compute gradients\n",
    "4. Print the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "x = torch.tensor([2.0])\n",
    "target = torch.tensor([5.0])\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Your code here\n",
    "z = # TODO: compute z = x * w + b\n",
    "loss = # TODO: compute loss = (z - target)**2\n",
    "\n",
    "# TODO: call backward to compute gradients\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Gradient w.r.t. w: {w.grad}\")\n",
    "print(f\"Gradient w.r.t. b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building Neural Networks\n",
    "\n",
    "In PyTorch, we build neural networks by subclassing `torch.nn.Module`. This gives us:\n",
    "- Automatic parameter tracking\n",
    "- Easy layer composition\n",
    "- Built-in training/evaluation modes\n",
    "\n",
    "### Anatomy of a PyTorch Model\n",
    "\n",
    "```python\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass here\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(num_inputs, 30),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(30, 20),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(20, num_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Create a model\n",
    "model = NeuralNetwork(num_inputs=50, num_outputs=3)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "To use the model, we pass data through it. The model returns **logits** (raw scores), which we can convert to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create random input (1 sample, 50 features)\n",
    "X = torch.rand((1, 50))\n",
    "\n",
    "# Forward pass - get logits\n",
    "logits = model(X)\n",
    "print(f\"Logits: {logits}\\n\")\n",
    "\n",
    "# Convert to probabilities with softmax\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "print(f\"Probabilities: {probabilities}\")\n",
    "print(f\"Sum of probabilities: {probabilities.sum():.4f}\")\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Build Your Own Network\n",
    "\n",
    "**TODO**: Create a neural network with:\n",
    "- Input size: 10\n",
    "- Hidden layer 1: 64 neurons with ReLU activation\n",
    "- Hidden layer 2: 32 neurons with ReLU activation  \n",
    "- Output size: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define your layers using nn.Sequential\n",
    "        self.layers = nn.Sequential(\n",
    "            # Your layers here\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Create and test your model\n",
    "my_model = MyNetwork()\n",
    "print(my_model)\n",
    "\n",
    "# Test with random input\n",
    "test_input = torch.rand((1, 10))\n",
    "output = my_model(test_input)\n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Data Loading\n",
    "\n",
    "PyTorch provides `Dataset` and `DataLoader` classes for efficient data loading:\n",
    "\n",
    "1. **Dataset**: Defines how to access individual samples\n",
    "2. **DataLoader**: Batches data, shuffles, and loads in parallel\n",
    "\n",
    "### Creating a Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Return one sample\n",
    "        return self.features[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return dataset size\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create toy data\n",
    "X_train = torch.tensor([[-1.2, 3.1],\n",
    "                        [-0.9, 2.9],\n",
    "                        [-0.5, 2.6],\n",
    "                        [2.3, -1.1],\n",
    "                        [2.7, -1.5]])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([[-0.8, 2.8],\n",
    "                       [2.6, -1.6]])\n",
    "y_test = torch.tensor([0, 1])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ToyDataset(X_train, y_train)\n",
    "test_dataset = ToyDataset(X_test, y_test)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducible shuffling\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True  # Drop last incomplete batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Iterate over batches\n",
    "print(\"Training batches:\")\n",
    "for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}: Features shape={features.shape}, Labels={labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: The Training Loop\n",
    "\n",
    "Now we combine everything: model, data, loss function, and optimizer.\n",
    "\n",
    "### Key Components:\n",
    "1. **Model**: The neural network\n",
    "2. **Loss Function**: Measures prediction error (e.g., cross-entropy)\n",
    "3. **Optimizer**: Updates weights to minimize loss (e.g., SGD, Adam)\n",
    "4. **Training Loop**: Iterate over data, compute loss, update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create model (2 inputs, 2 outputs for binary classification)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "# Define optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "              f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After training, we evaluate the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for features, labels in dataloader:\n",
    "            logits = model(features)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "train_acc = compute_accuracy(model, train_loader)\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complete Training Loop\n",
    "\n",
    "**TODO**: Fill in the missing parts of the training loop below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh model\n",
    "torch.manual_seed(42)\n",
    "student_model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "# TODO: Create an Adam optimizer with learning rate 0.01\n",
    "optimizer = # TODO\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        # TODO: Forward pass - compute logits\n",
    "        logits = # TODO\n",
    "        \n",
    "        # TODO: Compute cross-entropy loss\n",
    "        loss = # TODO\n",
    "        \n",
    "        # TODO: Zero gradients\n",
    "        # TODO\n",
    "        \n",
    "        # TODO: Backward pass\n",
    "        # TODO\n",
    "        \n",
    "        # TODO: Update weights\n",
    "        # TODO\n",
    "    \n",
    "    # Print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "final_acc = compute_accuracy(student_model, test_loader)\n",
    "print(f\"\\nFinal Test Accuracy: {final_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Saving and Loading Models\n",
    "\n",
    "After training, we want to save our model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"my_model.pth\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "loaded_model.load_state_dict(torch.load(\"my_model.pth\", weights_only=True))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Verify it works\n",
    "test_acc = compute_accuracy(loaded_model, test_loader)\n",
    "print(f\"Loaded model test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: GPU Training (Optional)\n",
    "\n",
    "If you have a GPU available, you can significantly speed up training.\n",
    "\n",
    "### Key Concepts:\n",
    "1. Move model to GPU: `model.to(device)`\n",
    "2. Move data to GPU: `data.to(device)`\n",
    "3. All tensors must be on the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model and move to GPU\n",
    "torch.manual_seed(123)\n",
    "gpu_model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "gpu_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(gpu_model.parameters(), lr=0.5)\n",
    "\n",
    "# Training loop with GPU\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    gpu_model.train()\n",
    "    \n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        # Move data to GPU\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = gpu_model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "              f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: GPU Training\n",
    "\n",
    "**TODO**: Modify the accuracy computation function to work with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_gpu(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            # TODO: Move features and labels to device\n",
    "            features, labels = # TODO\n",
    "            \n",
    "            logits = model(features)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Test your function\n",
    "test_acc = compute_accuracy_gpu(gpu_model, test_loader, device)\n",
    "print(f\"GPU Model Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Putting It All Together - Real Example\n",
    "\n",
    "Let's create a complete example with a slightly more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate 1000 samples with 20 features\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "\n",
    "# Random features and labels\n",
    "X = torch.randn(n_samples, n_features)\n",
    "y = torch.randint(0, n_classes, (n_samples,))\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "n_train = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {n_features}\")\n",
    "print(f\"Classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and loaders\n",
    "train_dataset = ToyDataset(X_train, y_train)\n",
    "test_dataset = ToyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "torch.manual_seed(42)\n",
    "model = NeuralNetwork(num_inputs=n_features, num_outputs=n_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate every epoch\n",
    "    train_acc = compute_accuracy_gpu(model, train_loader, device)\n",
    "    test_acc = compute_accuracy_gpu(model, test_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Loss: {avg_loss:.4f} | \"\n",
    "          f\"Train Acc: {train_acc*100:.2f}% | \"\n",
    "          f\"Test Acc: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the essential PyTorch concepts:\n",
    "\n",
    "### âœ… Key Takeaways:\n",
    "\n",
    "1. **Tensors**: Multi-dimensional arrays (similar to NumPy) with GPU support\n",
    "2. **Autograd**: Automatic gradient computation for backpropagation\n",
    "3. **nn.Module**: Base class for building neural networks\n",
    "4. **DataLoader**: Efficient batching and data loading\n",
    "5. **Training Loop**: \n",
    "   - Forward pass â†’ Compute loss\n",
    "   - Backward pass â†’ Compute gradients\n",
    "   - Optimizer step â†’ Update weights\n",
    "6. **GPU Support**: Simple `.to(device)` for acceleration\n",
    "\n",
    "### ðŸŽ¯ Next Steps:\n",
    "\n",
    "- Experiment with different architectures\n",
    "- Try different optimizers (Adam, AdamW, etc.)\n",
    "- Explore real datasets (MNIST, CIFAR-10)\n",
    "- Learn about CNNs, RNNs, Transformers\n",
    "- Build your own projects!\n",
    "\n",
    "### ðŸ“š Further Resources:\n",
    "\n",
    "- **Official PyTorch Tutorials**: https://pytorch.org/tutorials/\n",
    "- **PyTorch Documentation**: https://pytorch.org/docs/\n",
    "- **Original Tutorial**: https://sebastianraschka.com/teaching/pytorch-1h/\n",
    "- **Books**:\n",
    "  - *Deep Learning with PyTorch* by Stevens, Antiga, and Viehmann\n",
    "  - *Machine Learning with PyTorch and Scikit-Learn* by Raschka et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Exercise: Build a Complete Project\n",
    "\n",
    "**Challenge**: Create a neural network to classify the Iris dataset\n",
    "\n",
    "**Steps**:\n",
    "1. Load the Iris dataset (sklearn)\n",
    "2. Create PyTorch datasets and dataloaders\n",
    "3. Build a neural network\n",
    "4. Train for multiple epochs\n",
    "5. Evaluate and report accuracy\n",
    "\n",
    "**Starter code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus challenge - Iris classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} training, {len(X_test)} test samples\")\n",
    "print(f\"Features: {X_train.shape[1]}, Classes: {len(iris.target_names)}\")\n",
    "\n",
    "# TODO: Complete the rest of the implementation\n",
    "# 1. Create datasets and dataloaders\n",
    "# 2. Define a neural network\n",
    "# 3. Train the model\n",
    "# 4. Evaluate accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
