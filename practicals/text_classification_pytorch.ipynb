{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Classical ML, RNN, and CNN\n",
    "## NLP at AIMS South Africa 2025\n",
    "### Based on Lecture 05: Sequence Modeling\n",
    "\n",
    "This notebook demonstrates sentiment classification using three approaches:\n",
    "1. **Classical ML** (baseline for comparison)\n",
    "2. **Recurrent Neural Networks (RNN/LSTM)**\n",
    "3. **Convolutional Neural Networks (CNN)**\n",
    "\n",
    "**Dataset**: HausaNLP/AfriSenti-Twitter (Sentiment analysis on African languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets torch scikit-learn pandas numpy matplotlib seaborn torchtext -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AfriSenti-Twitter dataset directly from GitHub\n",
    "# Note: HuggingFace no longer supports dataset scripts, so we load directly from the repository\n",
    "\n",
    "LANG = 'hau'  # Hausa language\n",
    "BASE_URL = f\"https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/{LANG}\"\n",
    "\n",
    "# Load the data using pandas\n",
    "train_df = pd.read_csv(f\"{BASE_URL}/train.tsv\", sep='\\t')\n",
    "val_df = pd.read_csv(f\"{BASE_URL}/dev.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(f\"{BASE_URL}/test.tsv\", sep='\\t')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {val_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(\"\\nColumns:\", train_df.columns.tolist())\n",
    "print(\"\\nSample from training set:\")\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "train_texts = train_df['tweet'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "val_texts = val_df['tweet'].tolist()\n",
    "val_labels = val_df['label'].tolist()\n",
    "\n",
    "test_texts = test_df['tweet'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# Check label distribution\n",
    "label_names = ['positive', 'neutral', 'negative']\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    count = train_labels.count(name)\n",
    "    print(f\"{name}: {count} ({count/len(train_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classical Machine Learning (Baseline)\n",
    "\n",
    "**Why Classical ML?**\n",
    "- Fast to train\n",
    "- Interpretable\n",
    "- Good baseline for comparison\n",
    "\n",
    "**Limitation**: Treats text as bag-of-words (ignores word order and context)\n",
    "\n",
    "We'll use TF-IDF features with three classifiers:\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_val = vectorizer.transform(val_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classical ML models\n",
    "classical_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Linear SVM': LinearSVC(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "classical_results = {}\n",
    "\n",
    "for name, model in classical_models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classical_results[name] = accuracy\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway: Classical ML Limitations\n",
    "\n",
    "**Example demonstrating bag-of-words limitation:**\n",
    "- \"Football makes people happy.\" \n",
    "- \"People make football happy.\"\n",
    "\n",
    "Both sentences contain identical words, so bag-of-words models treat them as the same, even though their meanings differ. This is where sequence models (RNN/CNN) excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "class Vocabulary:\n",
    "    def __init__(self, max_size=5000):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Get most common words\n",
    "        most_common = word_counts.most_common(self.max_size - 2)  # -2 for PAD and UNK\n",
    "        \n",
    "        # Build mappings\n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to indices\"\"\"\n",
    "        words = text.lower().split()\n",
    "        return [self.word2idx.get(word, 1) for word in words]  # 1 is <UNK>\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(max_size=5000)\n",
    "vocab.build_vocab(train_texts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample encoding: {train_texts[0][:50]}...\")\n",
    "print(f\"Encoded: {vocab.encode(train_texts[0])[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.vocab.encode(self.texts[idx])\n",
    "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return texts, labels\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, vocab)\n",
    "val_dataset = TextDataset(val_texts, val_labels, vocab)\n",
    "test_dataset = TextDataset(test_texts, test_labels, vocab)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recurrent Neural Network (RNN/LSTM)\n",
    "\n",
    "**Why RNN for Text?**\n",
    "- Processes input **sequentially** (word by word)\n",
    "- Maintains **memory** of previous words via hidden states\n",
    "- Captures **long-range dependencies** and temporal relationships\n",
    "- Handles **variable-length** input naturally\n",
    "\n",
    "**LSTM** (Long Short-Term Memory) is used to solve the vanishing gradient problem in vanilla RNNs.\n",
    "\n",
    "**Architecture**:\n",
    "1. Embedding Layer: Converts words to dense vectors\n",
    "2. LSTM Layer: Processes sequences and captures temporal patterns\n",
    "3. Dense Layer: Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN/LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           batch_first=True, dropout=dropout if n_layers > 1 else 0,\n",
    "                           bidirectional=False)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # LSTM output\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)  # lstm_out: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        hidden = hidden[-1]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(hidden))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Create model\n",
    "rnn_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(rnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN model\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Training RNN/LSTM model...\\n\")\n",
    "rnn_train_losses, rnn_train_accs, rnn_val_losses, rnn_val_accs = train_model(\n",
    "    rnn_model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_preds, rnn_labels = evaluate_model(rnn_model, test_loader)\n",
    "rnn_accuracy = accuracy_score(rnn_labels, rnn_preds)\n",
    "\n",
    "print(f\"\\nRNN Test Accuracy: {rnn_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(rnn_labels, rnn_preds, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Network (CNN)\n",
    "\n",
    "**Why CNN for Text?**\n",
    "- Extracts **local patterns** (n-grams) using filters\n",
    "- Efficient parallel processing (faster than RNN)\n",
    "- Good for capturing **local context** and phrases\n",
    "- Effective for **text classification** tasks\n",
    "\n",
    "**How it works:**\n",
    "1. Convolution filters slide over word embeddings\n",
    "2. Captures local features (2-5 word patterns)\n",
    "3. Max pooling extracts most important features\n",
    "4. Dense layer for classification\n",
    "\n",
    "**Architecture**:\n",
    "1. Embedding Layer: Word representations\n",
    "2. Conv1D Layer: Extract local n-gram features\n",
    "3. GlobalMaxPooling: Get most relevant features\n",
    "4. Dense Layers: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, output_dim, dropout=0.5):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Multiple convolutional layers with different kernel sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                     out_channels=num_filters,\n",
    "                     kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(filter_sizes) * num_filters, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedded = self.embedding(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Transpose for conv1d: [batch_size, embedding_dim, seq_len]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply convolution and pooling for each filter size\n",
    "        conved = [self.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conved[i]: [batch_size, num_filters, seq_len - filter_sizes[i] + 1]\n",
    "        \n",
    "        # Max pooling over time dimension\n",
    "        pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n",
    "        # pooled[i]: [batch_size, num_filters]\n",
    "        \n",
    "        # Concatenate pooled features\n",
    "        cat = torch.cat(pooled, dim=1)  # [batch_size, num_filters * len(filter_sizes)]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(cat))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "NUM_FILTERS = 128\n",
    "FILTER_SIZES = [3, 4, 5]  # Multiple filter sizes to capture different n-grams\n",
    "\n",
    "# Create model\n",
    "cnn_model = CNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(cnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Training CNN model...\\n\")\n",
    "cnn_train_losses, cnn_train_accs, cnn_val_losses, cnn_val_accs = train_model(\n",
    "    cnn_model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN model\n",
    "cnn_preds, cnn_labels = evaluate_model(cnn_model, test_loader)\n",
    "cnn_accuracy = accuracy_score(cnn_labels, cnn_preds)\n",
    "\n",
    "print(f\"\\nCNN Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(cnn_labels, cnn_preds, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = classical_results.copy()\n",
    "all_results['RNN (LSTM)'] = rnn_accuracy\n",
    "all_results['CNN'] = cnn_accuracy\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for model_name, accuracy in sorted(all_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model_name:25s}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "models = list(all_results.keys())\n",
    "accuracies = list(all_results.values())\n",
    "colors = ['skyblue']*3 + ['lightcoral', 'lightgreen']  # Different colors for classical vs neural\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=colors, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Model', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.title('Performance Comparison: Classical ML vs Neural Networks', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for neural models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RNN Loss\n",
    "axes[0, 0].plot(rnn_train_losses, label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(rnn_val_losses, label='Val Loss', marker='s')\n",
    "axes[0, 0].set_title('RNN/LSTM Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# RNN Accuracy\n",
    "axes[0, 1].plot(rnn_train_accs, label='Train Accuracy', marker='o')\n",
    "axes[0, 1].plot(rnn_val_accs, label='Val Accuracy', marker='s')\n",
    "axes[0, 1].set_title('RNN/LSTM Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# CNN Loss\n",
    "axes[1, 0].plot(cnn_train_losses, label='Train Loss', marker='o')\n",
    "axes[1, 0].plot(cnn_val_losses, label='Val Loss', marker='s')\n",
    "axes[1, 0].set_title('CNN Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# CNN Accuracy\n",
    "axes[1, 1].plot(cnn_train_accs, label='Train Accuracy', marker='o')\n",
    "axes[1, 1].plot(cnn_val_accs, label='Val Accuracy', marker='s')\n",
    "axes[1, 1].set_title('CNN Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for neural models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RNN confusion matrix\n",
    "cm_rnn = confusion_matrix(rnn_labels, rnn_preds)\n",
    "sns.heatmap(cm_rnn, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[0])\n",
    "axes[0].set_title('RNN/LSTM Confusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# CNN confusion matrix\n",
    "cm_cnn = confusion_matrix(cnn_labels, cnn_preds)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=label_names, yticklabels=label_names, ax=axes[1])\n",
    "axes[1].set_title('CNN Confusion Matrix', fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights & Comparison\n",
    "\n",
    "### Classical ML (TF-IDF + Classifiers)\n",
    "**Pros:**\n",
    "- Fast training and inference\n",
    "- Low computational requirements\n",
    "- Interpretable (feature importance)\n",
    "- Good baseline performance\n",
    "\n",
    "**Cons:**\n",
    "- Ignores word order (bag-of-words)\n",
    "- Cannot capture long-range dependencies\n",
    "- Limited semantic understanding\n",
    "- Fixed-length feature vectors\n",
    "\n",
    "### RNN/LSTM\n",
    "**Pros:**\n",
    "- Processes sequences step-by-step\n",
    "- Captures temporal relationships\n",
    "- Handles variable-length input\n",
    "- Maintains context via hidden states\n",
    "- Good for long-range dependencies\n",
    "\n",
    "**Cons:**\n",
    "- Sequential processing (slower)\n",
    "- Vanishing/exploding gradients (mitigated by LSTM)\n",
    "- More parameters to train\n",
    "\n",
    "### CNN\n",
    "**Pros:**\n",
    "- Parallel processing (faster than RNN)\n",
    "- Captures local patterns (n-grams)\n",
    "- Fewer parameters than LSTM\n",
    "- Effective for classification tasks\n",
    "\n",
    "**Cons:**\n",
    "- Limited global context\n",
    "- Fixed kernel sizes\n",
    "- May miss long-range dependencies\n",
    "\n",
    "### When to Use Each?\n",
    "- **Classical ML**: Quick baseline, limited data, need interpretability\n",
    "- **RNN/LSTM**: Long documents, need full sequential context, sequence generation\n",
    "- **CNN**: Fast classification, local patterns important, shorter texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on New Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, model_name, vocab, device):\n",
    "    \"\"\"Predict sentiment for a given text using neural models\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    encoded = vocab.encode(text)\n",
    "    tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    \n",
    "    print(f\"\\n{model_name} Prediction:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {label_names[predicted_class]} (confidence: {probabilities[predicted_class]:.4f})\")\n",
    "    print(f\"Class probabilities:\")\n",
    "    for i, name in enumerate(label_names):\n",
    "        print(f\"  {name}: {probabilities[i]:.4f}\")\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    test_texts[0],\n",
    "    test_texts[10],\n",
    "    test_texts[20]\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    predict_sentiment(example, rnn_model, \"RNN/LSTM\", vocab, device)\n",
    "    predict_sentiment(example, cnn_model, \"CNN\", vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated three approaches to text classification:\n",
    "\n",
    "1. **Classical ML** provides a strong baseline using TF-IDF features but ignores word order\n",
    "2. **RNN/LSTM** captures sequential patterns and long-range dependencies through recurrent connections\n",
    "3. **CNN** efficiently extracts local n-gram features using convolution filters\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Classical ML is fast but limited by bag-of-words assumption\n",
    "- RNNs excel at modeling sequential dependencies but are computationally expensive\n",
    "- CNNs offer a good balance between performance and efficiency for text classification\n",
    "- The choice depends on your specific task, data, and computational constraints\n",
    "\n",
    "**Implementation Notes:**\n",
    "- PyTorch provides flexible model building with custom architectures\n",
    "- DataLoader handles batching and padding efficiently\n",
    "- GPU acceleration significantly speeds up training\n",
    "- Proper evaluation metrics are crucial for understanding model performance\n",
    "\n",
    "**Next Steps:**\n",
    "- Try bidirectional LSTM (Bi-LSTM) for better context capture\n",
    "- Experiment with multiple CNN filter sizes simultaneously\n",
    "- Use pre-trained embeddings (Word2Vec, GloVe, FastText)\n",
    "- Explore hybrid models (CNN + LSTM)\n",
    "- Consider attention mechanisms\n",
    "- Try transformer-based models (BERT, RoBERTa) for state-of-the-art performance (Next Lecture!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploration\n",
    "\n",
    "**For more example on understanding and implementing sequence classification, check this repository:**\n",
    "\n",
    "```https://github.com/bentrevett/pytorch-sentiment-analysis```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
